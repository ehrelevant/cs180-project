{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12000005",
   "metadata": {},
   "source": [
    "This notebook uses the model from https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "\n",
    "All seeds have already been set to 42, however if you will run torch using cuda, expect there to still be slight variations in the results if you rerun it. This is due to torch using non-deterministic algorithms (these are 3x faster than deterministic ones for this model at least from my testing)\n",
    "\n",
    "Based on the code from \"may15_VIT3.ipynb\" in old\n",
    "\n",
    "Overall:\n",
    "- uses 8 epochs, should take 30 minutes at most\n",
    "    - could be improved setting it so that the model trains forever (like say until 20 epochs) until the validation loss plateus\n",
    "- has L2 regularization\n",
    "    - ctrl+f \"weight_decay\" to find the line that sets this\n",
    "- NO dropouts \n",
    "    - from testing may15_VIT2 VS may15_VIT3, applying 0.2 dropout made the validation loss slightly bigger, but maybe different values of dropout (0.1, or 0.5) would do better\n",
    "- uses the initial image preprocessing transforms from may13_VIT1.ipynb\n",
    "    - this achieves the lowest training loss with lower validation loss as well, but it has the biggest gap (difference) between training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01cae6",
   "metadata": {},
   "source": [
    "Useful documentation links for finding what parameters we can change:\n",
    "\n",
    "HuggingFace:\n",
    "- https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.ViTConfig\n",
    "\n",
    "PyTorch:\n",
    "- https://docs.pytorch.org/vision/0.9/transforms.html\n",
    "- https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce118d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if running in Google Colab, do these\n",
    "# !pip install datasets=='3.5.1' evaluate torch torchvision transformers Pillow numpy scikit-learn 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99de451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import os\n",
    "\"\"\"\n",
    ".venv/Scripts/activate\n",
    "\n",
    "python -m image_process\n",
    "\"\"\"\n",
    "base_output_dir = \"model\" ## if you wanna save different models, just make a new git branch, saving a VIT model takes up A LOT of space\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "dataset = load_dataset(\"potato_train/train\")\n",
    "filenames_ds = load_dataset(\"potato_train/train\").cast_column(\"image\", Image(decode=False))\n",
    "\n",
    "filename_col = [x['image']['path'].split('\\\\')[-1] for x in filenames_ds['train']]\n",
    "dataset['train'] = dataset['train'].add_column(\"filename\", filename_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadebc02",
   "metadata": {},
   "source": [
    "We retrieve the feature extractor from our desired VIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f143290-9281-4f29-92d0-f9013f809519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "# import model\n",
    "model_id = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "# feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e548a6",
   "metadata": {},
   "source": [
    "These are the steps used to preprocess the images and to perform data augmentation on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2617bbc7-5778-475f-a8fd-3be9fa1d5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    # RandomRotation,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    RandomAffine,\n",
    "    # Pad,\n",
    "    # RandomCrop\n",
    ")\n",
    "from PIL import Image  # Import PIL for RandomAffine's resample\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seeds(seed)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])\n",
    "\n",
    "training_transforms = Compose([\n",
    "    Resize(size),\n",
    "    CenterCrop(size),\n",
    "    # RandomRotation((-30, 30)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), interpolation=Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def training_image_preprocess(batch):\n",
    "    batch[\"pixel_values\"] = torch.stack([training_transforms(img) for img in batch[\"image\"]])\n",
    "    return batch\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = feature_extractor(batch['image'], return_tensors='pt')\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cfd97ad-58b8-4429-8bac-f0639a92b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94175340",
   "metadata": {},
   "source": [
    "We split the dataset. 80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4974f0-9cdb-4fa2-90ba-67a69a98b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "dataset_train = train_test_split[\"train\"]\n",
    "dataset_test = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb414c3b-07e5-4478-a882-21dbe846e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " ClassLabel(names=['Bacteria', 'Fungi', 'Healthy', 'Pest', 'Phytopthora', 'Virus'], id=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5514339e-7fcc-4c40-aa3c-8d198423a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = dataset_train.with_transform(training_image_preprocess)\n",
    "prepared_test = dataset_test.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb978-d838-4ba0-8259-32618f3a1b83",
   "metadata": {},
   "source": [
    "Save images of preprocessed images (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8ef810-fc8e-4ff6-af4d-138072228ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def save_unnormalized_images(prepared_dataset, raw_dataset, directory: str):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for index, item in enumerate(prepared_dataset):\n",
    "        if index >= 10:\n",
    "            break\n",
    "        pixel_values = item[\"pixel_values\"]\n",
    "        image = to_pil_image(pixel_values)\n",
    "        label_filename = raw_dataset[index][\"filename\"]\n",
    "\n",
    "        name_without_extension, extension = os.path.splitext(label_filename)\n",
    "        filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        image.save(filepath)\n",
    "\n",
    "\n",
    "save_unnormalized_images(prepared_train, dataset_train, f\"{base_output_dir}/preprocessed_train_images\")\n",
    "save_unnormalized_images(prepared_test, dataset_test, f\"{base_output_dir}/preprocessed_test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ab1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=p.label_ids,\n",
    "        )\n",
    "    )\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=p.label_ids, average=\"weighted\"))\n",
    "    return results\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7152c39",
   "metadata": {},
   "source": [
    "Training Arguments, you can apply stuff like \n",
    "- L1 or L2 regularization\n",
    "- hidden dropouts\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2653db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dropout=0.0, attention dropout=0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments, ViTConfig\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=base_output_dir,\n",
    "  per_device_train_batch_size=16,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=8, ## CHANGE NUMBER OF EPOCHS here\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=5e-5, ## we could try applying learning rate cosine smthg keme\n",
    "  save_total_limit=2,\n",
    "  seed=seed,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    "  weight_decay=0.01,  # Add this line to apply L2 regularization\n",
    ")\n",
    "config = ViTConfig.from_pretrained(model_id)\n",
    "config.num_labels = len(dataset_train.features['label'].names)\n",
    "# If you want to change it (do this BEFORE loading the model with from_pretrained):\n",
    "# config.hidden_dropout_prob = 0.2\n",
    "# config.attention_probs_dropout_prob = 0.2\n",
    "print(f\"hidden dropout={config.hidden_dropout_prob}, attention dropout={config.attention_probs_dropout_prob}\")\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_id,  # classification head\n",
    "    config=config,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b483da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/1632 00:04 < 43:05, 0.63 it/s, Epoch 0.03/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m trainer.save_model() \u001b[38;5;66;03m# save tokenizer with the model\u001b[39;00m\n\u001b[32m      4\u001b[39m trainer.log_metrics(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, train_results.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\accelerate\\data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    575\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2781\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2780\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2781\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2782\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:410\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:535\u001b[39m, in \u001b[36mCustomFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m    534\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.python_arrow_extractor().extract_batch(pa_table)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_features_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:229\u001b[39m, in \u001b[36mPythonFeaturesDecoder.decode_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features \u001b[38;5;28;01melse\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\features\\features.py:2144\u001b[39m, in \u001b[36mFeatures.decode_batch\u001b[39m\u001b[34m(self, batch, token_per_repo_id)\u001b[39m\n\u001b[32m   2140\u001b[39m decoded_batch = {}\n\u001b[32m   2141\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch.items():\n\u001b[32m   2142\u001b[39m     decoded_batch[column_name] = (\n\u001b[32m   2143\u001b[39m         [\n\u001b[32m-> \u001b[39m\u001b[32m2144\u001b[39m             \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2145\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2146\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2147\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[32m   2148\u001b[39m         ]\n\u001b[32m   2149\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._column_requires_decoding[column_name]\n\u001b[32m   2150\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[32m   2151\u001b[39m     )\n\u001b[32m   2152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\features\\features.py:1414\u001b[39m, in \u001b[36mdecode_nested_example\u001b[39m\u001b[34m(schema, obj, token_per_repo_id)\u001b[39m\n\u001b[32m   1411\u001b[39m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[32m   1412\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode_example\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\datasets\\features\\image.py:169\u001b[39m, in \u001b[36mImage.decode_example\u001b[39m\u001b[34m(self, value, token_per_repo_id)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_local_path(path):\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m         image = \u001b[43mPIL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m         source_url = path.split(\u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\PIL\\Image.py:3505\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3502\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3506\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "trainer.save_model() # save tokenizer with the model\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "\n",
    "trainer.save_state() # save the trainer state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a889ff",
   "metadata": {},
   "source": [
    "Retrieve the saved model from the directory, then run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Desktop\\CS_180\\cs180_project_TEMP SANDBOX\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy               =      0.893\n",
      "  eval_f1                     =     0.8927\n",
      "  eval_loss                   =     0.3716\n",
      "  eval_model_preparation_time =     0.0019\n",
      "  eval_runtime                = 0:00:22.92\n",
      "  eval_samples_per_second     =      23.64\n",
      "  eval_steps_per_second       =      2.966\n",
      "{'eval_loss': 0.3715899884700775, 'eval_model_preparation_time': 0.0019, 'eval_accuracy': 0.8929889298892989, 'eval_f1': 0.8926797071412873, 'eval_runtime': 22.9275, 'eval_samples_per_second': 23.64, 'eval_steps_per_second': 2.966}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(base_output_dir)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(base_output_dir)\n",
    "\n",
    "# Define the Trainer for evaluation\n",
    "# (if you don't wanna load the model from the directory and use the trained model directly, comment out the trainer line here)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
