{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12000005",
   "metadata": {},
   "source": [
    "This notebook uses the model from https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "\n",
    "All seeds have already been set to 42, however if you will run torch using cuda, expect there to still be slight variations in the results if you rerun it. This is due to torch using non-deterministic algorithms (these are 3x faster than deterministic ones for this model at least from my testing)\n",
    "\n",
    "Based on the code from \"may15_VIT3.ipynb\" in old\n",
    "\n",
    "Overall:\n",
    "- uses 8 epochs, should take 30 minutes at most\n",
    "    - could be improved setting it so that the model trains forever (like say until 20 epochs) until the validation loss plateus\n",
    "- has L2 regularization\n",
    "    - ctrl+f \"weight_decay\" to find the line that sets this\n",
    "- NO dropouts \n",
    "    - from testing may15_VIT2 VS may15_VIT3, applying 0.2 dropout made the validation loss slightly bigger, but maybe different values of dropout (0.1, or 0.5) would do better\n",
    "- uses the initial image preprocessing transforms from may13_VIT1.ipynb\n",
    "    - this achieves the lowest training loss with lower validation loss as well, but it has the biggest gap (difference) between training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01cae6",
   "metadata": {},
   "source": [
    "Useful documentation links for finding what parameters we can change:\n",
    "\n",
    "HuggingFace:\n",
    "- https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.ViTConfig\n",
    "\n",
    "PyTorch:\n",
    "- https://docs.pytorch.org/vision/0.9/transforms.html\n",
    "- https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eeeb8d",
   "metadata": {},
   "source": [
    "May 16, changes by justin\n",
    "- testing BEiT again with same params but with more patience to train for more epochs\n",
    "- results: epoch 4 (out of 17 epochs) surprisingly has the lowest evaluation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce118d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if running in Google Colab, do these\n",
    "# !pip install datasets=='3.5.1' evaluate torch torchvision transformers Pillow numpy scikit-learn 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99de451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import os\n",
    "\"\"\"\n",
    ".venv/Scripts/activate\n",
    "\n",
    "python -m image_process\n",
    "\"\"\"\n",
    "base_output_dir = \"model\" ## if you wanna save different models, just make a new git branch, saving a VIT model takes up A LOT of space\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "dataset = load_dataset(\"potato_train/train\")\n",
    "filenames_ds = load_dataset(\"potato_train/train\").cast_column(\"image\", Image(decode=False))\n",
    "\n",
    "filename_col = [x['image']['path'].split('\\\\')[-1] for x in filenames_ds['train']]\n",
    "dataset['train'] = dataset['train'].add_column(\"filename\", filename_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadebc02",
   "metadata": {},
   "source": [
    "We retrieve the feature extractor from our desired VIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f143290-9281-4f29-92d0-f9013f809519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `BeitImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BeitImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"do_center_crop\": false,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"BeitImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BeitImageProcessor\n",
    "\n",
    "# import model\n",
    "model_id = 'microsoft/beit-base-patch16-224-pt22k-ft22k'\n",
    "feature_extractor = BeitImageProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e548a6",
   "metadata": {},
   "source": [
    "These are the steps used to preprocess the images and to perform data augmentation on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2617bbc7-5778-475f-a8fd-3be9fa1d5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomRotation,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    RandomAffine,\n",
    "    # Pad,\n",
    "    # RandomCrop\n",
    ")\n",
    "from PIL import Image  # Import PIL for RandomAffine's resample\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seeds(seed)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])\n",
    "\n",
    "training_transforms = Compose([\n",
    "    Resize(size),\n",
    "    CenterCrop(size),\n",
    "    RandomRotation((-30, 30)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), interpolation=Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def training_image_preprocess(batch):\n",
    "    batch[\"pixel_values\"] = torch.stack([training_transforms(img) for img in batch[\"image\"]])\n",
    "    return batch\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = feature_extractor(batch['image'], return_tensors='pt')\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfd97ad-58b8-4429-8bac-f0639a92b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94175340",
   "metadata": {},
   "source": [
    "We split the dataset. 80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4974f0-9cdb-4fa2-90ba-67a69a98b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "dataset_train = train_test_split[\"train\"]\n",
    "dataset_test = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb414c3b-07e5-4478-a882-21dbe846e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " ClassLabel(names=['Bacteria', 'Fungi', 'Healthy', 'Pest', 'Phytopthora', 'Virus'], id=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5514339e-7fcc-4c40-aa3c-8d198423a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = dataset_train.with_transform(training_image_preprocess)\n",
    "prepared_test = dataset_test.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb978-d838-4ba0-8259-32618f3a1b83",
   "metadata": {},
   "source": [
    "Save images of preprocessed images (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8ef810-fc8e-4ff6-af4d-138072228ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def save_unnormalized_images(prepared_dataset, raw_dataset, directory: str):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for index, item in enumerate(prepared_dataset):\n",
    "        if index >= 10:\n",
    "            break\n",
    "        pixel_values = item[\"pixel_values\"]\n",
    "        image = to_pil_image(pixel_values)\n",
    "        label_filename = raw_dataset[index][\"filename\"]\n",
    "\n",
    "        name_without_extension, extension = os.path.splitext(label_filename)\n",
    "        filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        image.save(filepath)\n",
    "\n",
    "\n",
    "save_unnormalized_images(prepared_train, dataset_train, f\"{base_output_dir}/preprocessed_train_images\")\n",
    "save_unnormalized_images(prepared_test, dataset_test, f\"{base_output_dir}/preprocessed_test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ab1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=p.label_ids,\n",
    "        )\n",
    "    )\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=p.label_ids, average=\"weighted\"))\n",
    "    return results\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7152c39",
   "metadata": {},
   "source": [
    "Training Arguments, you can apply stuff like \n",
    "- L1 or L2 regularization\n",
    "- hidden dropouts\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2653db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dropout=0.0, attention dropout=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BeitForImageClassification, Trainer, TrainingArguments, BeitConfig\n",
    "#from transformers.trainer_utils import IntervalStrategy, SchedulerType\n",
    "#from transformers.training_args import OptimizerNames\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import hf_xet\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=base_output_dir,\n",
    "  per_device_train_batch_size=16,\n",
    "  eval_strategy=\"epoch\",  # Evaluate at the end of each epoch for early stopping\n",
    "  save_strategy=\"epoch\",\n",
    "  num_train_epochs=50,  # Set a large number of epochs, early stopping will handle it\n",
    "  logging_steps=100,\n",
    "  learning_rate=5e-5, ## we could try applying learning rate cosine smthg keme\n",
    "  save_total_limit=2,\n",
    "  seed=seed,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    "  #weight_decay=0.01,  # Add this line to apply L2 regularization\n",
    "  #lr_scheduler_type=\"cosine\",  # Use cosine learning rate scheduler\n",
    "  #warmup_steps=int(0.1 * (len(prepared_train) / 16) * 100), # 10% of the first epoch for warmup\n",
    "  # ^ Adjust warmup_steps based on your dataset size and batch size\n",
    ")\n",
    "config = BeitConfig.from_pretrained(model_id)\n",
    "config.num_labels = len(dataset_train.features['label'].names)\n",
    "# If you want to change it (do this BEFORE loading the model with from_pretrained):\n",
    "# config.hidden_dropout_prob = 0.2\n",
    "# config.attention_probs_dropout_prob = 0.2\n",
    "print(f\"hidden dropout={config.hidden_dropout_prob}, attention dropout={config.attention_probs_dropout_prob}\")\n",
    "\n",
    "model = BeitForImageClassification.from_pretrained(\n",
    "    model_id,  # classification head\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b483da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2313' max='6800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2313/6800 2:44:53 < 5:20:09, 0.23 it/s, Epoch 17/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>0.492139</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.817253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.463522</td>\n",
       "      <td>0.830258</td>\n",
       "      <td>0.821772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.832103</td>\n",
       "      <td>0.826415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.336561</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>0.878881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.407215</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.862854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.498483</td>\n",
       "      <td>0.869004</td>\n",
       "      <td>0.861726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.530395</td>\n",
       "      <td>0.835793</td>\n",
       "      <td>0.834252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.432127</td>\n",
       "      <td>0.876384</td>\n",
       "      <td>0.876972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.578221</td>\n",
       "      <td>0.870849</td>\n",
       "      <td>0.871021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.473726</td>\n",
       "      <td>0.880074</td>\n",
       "      <td>0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.538168</td>\n",
       "      <td>0.881919</td>\n",
       "      <td>0.882330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.638812</td>\n",
       "      <td>0.870849</td>\n",
       "      <td>0.869813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>0.594886</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>0.878728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.604422</td>\n",
       "      <td>0.874539</td>\n",
       "      <td>0.874919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.628854</td>\n",
       "      <td>0.876384</td>\n",
       "      <td>0.877924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.564070</td>\n",
       "      <td>0.896679</td>\n",
       "      <td>0.896522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.625122</td>\n",
       "      <td>0.894834</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m trainer.save_model() \u001b[38;5;66;03m# save tokenizer with the model\u001b[39;00m\n\u001b[32m      4\u001b[39m trainer.log_metrics(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, train_results.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:1082\u001b[39m, in \u001b[36mBeitForImageClassification.forward\u001b[39m\u001b[34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1076\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1077\u001b[39m \u001b[33;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1078\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1079\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1080\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1081\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbeit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m pooled_output = outputs.pooler_output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1093\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:892\u001b[39m, in \u001b[36mBeitModel.forward\u001b[39m\u001b[34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[39m\n\u001b[32m    889\u001b[39m embedding_output, _ = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n\u001b[32m    890\u001b[39m resolution = pixel_values.shape[\u001b[32m2\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    901\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    902\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.layernorm(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:718\u001b[39m, in \u001b[36mBeitEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, resolution, return_dict)\u001b[39m\n\u001b[32m    708\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    709\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    710\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m         resolution,\n\u001b[32m    716\u001b[39m     )\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:539\u001b[39m, in \u001b[36mBeitLayer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    531\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    532\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    537\u001b[39m     resolution: Optional[Tuple[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    538\u001b[39m ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in BEiT, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    548\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:470\u001b[39m, in \u001b[36mBeitAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    462\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    463\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    468\u001b[39m     resolution: Optional[Tuple[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    469\u001b[39m ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    476\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:385\u001b[39m, in \u001b[36mBeitSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution)\u001b[39m\n\u001b[32m    383\u001b[39m     height, width = resolution\n\u001b[32m    384\u001b[39m     window_size = (height // \u001b[38;5;28mself\u001b[39m.config.patch_size, width // \u001b[38;5;28mself\u001b[39m.config.patch_size)\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     attn_bias = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Add shared relative position bias if provided.\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m relative_position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py:640\u001b[39m, in \u001b[36mBeitRelativePositionBias.forward\u001b[39m\u001b[34m(self, window_size, interpolate_pos_encoding, dim_size)\u001b[39m\n\u001b[32m    637\u001b[39m relative_position_bias = new_relative_position_bias_table[relative_position_index.view(-\u001b[32m1\u001b[39m)]\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# patch_size*num_patches_height, patch_size*num_patches_width, num_attention_heads\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m relative_position_bias = \u001b[43mrelative_position_bias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[38;5;66;03m# num_attention_heads, patch_size*num_patches_width, patch_size*num_patches_height\u001b[39;00m\n\u001b[32m    644\u001b[39m relative_position_bias = relative_position_bias.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "trainer.save_model() # save tokenizer with the model\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "\n",
    "trainer.save_state() # save the trainer state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a889ff",
   "metadata": {},
   "source": [
    "Retrieve the saved model from the directory, then run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e959365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy               =     0.8782\n",
      "  eval_f1                     =     0.8789\n",
      "  eval_loss                   =     0.3366\n",
      "  eval_model_preparation_time =      0.003\n",
      "  eval_runtime                = 0:00:25.77\n",
      "  eval_samples_per_second     =     21.025\n",
      "  eval_steps_per_second       =      2.638\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, BeitForImageClassification, BeitImageProcessor\n",
    "\n",
    "checkpoint = '544'\n",
    "model = BeitForImageClassification.from_pretrained(f'{base_output_dir}/checkpoint-{checkpoint}')\n",
    "feature_extractor = BeitImageProcessor.from_pretrained(f'{base_output_dir}/checkpoint-{checkpoint}')\n",
    "\n",
    "# Define the Trainer for evaluation\n",
    "# (if you don't wanna load the model from the directory and use the trained model directly, comment out the trainer line here)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b7d01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAIOCAYAAACyFKlTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdcBJREFUeJzt3Qd8E+UbB/BfunehZZRR9pa9lwKCFlAEcSLKBkGQJUOULYiKDBEBJwh/UHAhKAqIsgRRWaLIklX2aimUzuT+n+fFhKRpsaUJJXe/r5+zyd3lcteE5snzPu/7mjRN00BEREREitf1H0REREQkGBwRERER2WFwRERERGSHwRERERGRHQZHRERERHYYHBERERHZYXBEREREZIfBEREREZEdBkdEREREdhgcEVGuHDx4EPfffz/Cw8NhMpmwfPlylx7/6NGj6rgLFixw6XE9WfPmzdVCRO7B4IhIB/755x88++yzKFOmDAICAhAWFoYmTZrgrbfeQlJSklufu2vXrtizZw8mT56MRYsWoW7dutCLbt26qcBMfp+Z/R4lMJTtsrz55ps5Pv6pU6cwfvx47Nq1y0VnTESu4OOSoxBRnvn222/x2GOPwd/fH126dEHVqlWRmpqKzZs3Y/jw4fjrr7/w3nvvueW5JWDYunUrXn75ZQwYMMAtz1GyZEn1PL6+vsgLPj4+uHbtGlauXInHH3/cYdvixYtVMJqcnHxLx5bgaMKECShVqhRq1qyZ7cetWbPmlp6PiLKHwRGRBzty5AiefPJJFUD8+OOPKFKkiG1b//79cejQIRU8ucv58+fVz3z58rntOSQrIwFIXpGgU7Jwn3zyiVNwtGTJEjzwwAP44osvbsu5SJAWFBQEPz+/2/J8REbFZjUiD/bGG2/g6tWr+PDDDx0CI6ty5cph0KBBtvvp6el45ZVXULZsWfWhLxmLl156CSkpKQ6Pk/UPPvigyj7Vr19fBSfSZLdw4ULbPtIcJEGZkAyVBDHyOGtzlPW2PXmM7Gdv7dq1aNq0qQqwQkJCULFiRXVO/1VzJMHg3XffjeDgYPXY9u3b4++//870+SRIlHOS/aQ2qnv37irQyK6nnnoK3333HeLj423rfvvtN9WsJtsyunTpEoYNG4Zq1aqpa5JmuTZt2mD37t22fdavX4969eqp23I+1uY563VKTZFkAbdv34577rlHBUXW30vGmiNp2pTXKOP1x8TEIH/+/CpDRUTZx+CIyINJU48ELY0bN87W/r169cLYsWNRu3ZtzJgxA82aNcOUKVNU9ikjCSgeffRR3HfffZg2bZr6kJUAQ5rpRMeOHdUxRKdOnVS90cyZM3N0/nIsCcIkOJs4caJ6noceegg///zzTR/3ww8/qA/+c+fOqQBo6NCh2LJli8rwSDCVkWR8rly5oq5VbksAIs1Z2SXXKoHLl19+6ZA1qlSpkvpdZnT48GFVmC7XNn36dBU8Sl2W/L6tgUrlypXVNYs+ffqo358sEghZXbx4UQVV0uQmv9sWLVpken5SW1awYEEVJJnNZrXu3XffVc1vb7/9NooWLZrtayUiABoReaTLly9r8k+4ffv22dp/165dav9evXo5rB82bJha/+OPP9rWlSxZUq3buHGjbd25c+c0f39/7YUXXrCtO3LkiNpv6tSpDsfs2rWrOkZG48aNU/tbzZgxQ90/f/58ludtfY758+fb1tWsWVMrVKiQdvHiRdu63bt3a15eXlqXLl2cnq9Hjx4Ox3z44Ye1yMjILJ/T/jqCg4PV7UcffVRr2bKlum02m7WoqChtwoQJmf4OkpOT1T4Zr0N+fxMnTrSt++2335yuzapZs2Zq27x58zLdJou91atXq/0nTZqkHT58WAsJCdE6dOjwn9dIRM6YOSLyUAkJCepnaGhotvZftWqV+ilZFnsvvPCC+pmxNqlKlSqq2cpKMhPS5CVZEVex1ip9/fXXsFgs2XrM6dOnVe8uyWJFRETY1levXl1luazXaa9v374O9+W6JCtj/R1mhzSfSVPYmTNnVJOe/MysSU1Ik6WX1/U/r5LJkeeyNhnu2LEj288px5Emt+yQ4RSkx6JkoyTTJc1skj0iopxjcETkoaSORUhzUXYcO3ZMfWBLHZK9qKgoFaTIdnslSpRwOoY0rcXFxcFVnnjiCdUUJs19hQsXVs17y5Ytu2mgZD1PCTQykqaqCxcuIDEx8abXItchcnItbdu2VYHo0qVLVS81qRfK+Lu0kvOXJsfy5curAKdAgQIquPzjjz9w+fLlbD9nsWLFclR8LcMJSMAoweOsWbNQqFChbD+WiG5gcETkwcGR1JL8+eefOXpcxoLorHh7e2e6XtO0W34Oaz2MVWBgIDZu3KhqiJ555hkVPEjAJBmgjPvmRm6uxUqCHMnIfPzxx/jqq6+yzBqJV199VWXopH7of//7H1avXq0Kz++6665sZ8isv5+c2Llzp6rDElLjRES3hsERkQeTgl8ZAFLGGvov0rNMPpilh5W9s2fPql5Y1p5nriCZGfueXVYZs1NCslktW7ZUhct79+5Vg0lKs9VPP/2U5XWI/fv3O23bt2+fytJIDzZ3kIBIAhDJ1mVWxG71+eefq+Jp6UUo+0mTV6tWrZx+J9kNVLNDsmXSBCfNoVLgLT0ZpUcdEeUcgyMiDzZixAgVCEizlAQ5GUngJD2ZrM1CImOPMglKhIzX4yoyVIA0H0kmyL5WSDIuGbu8Z2QdDDHj8AJWMmSB7CMZHPtgQzJo0jvLep3uIAGPDIUwe/Zs1Rx5s0xVxqzUZ599hpMnTzqsswZxmQWSOTVy5EgcP35c/V7kNZWhFKT3Wla/RyLKGgeBJPJgEoRIl3JpipJ6G/sRsqVru3wgS+GyqFGjhvqwlNGy5cNYupX/+uuv6sO0Q4cOWXYTvxWSLZEP64cffhgDBw5UYwrNnTsXFSpUcChIluJhaVaTwEwyQtIkNGfOHBQvXlyNfZSVqVOnqi7ujRo1Qs+ePdUI2tJlXcYwkq797iJZrtGjR2croyfXJpkcGWZBmrikTkmGXcj4+km917x581Q9kwRLDRo0QOnSpXN0XpJpk9/buHHjbEMLzJ8/X42FNGbMGJVFIqIcyKQHGxF5mAMHDmi9e/fWSpUqpfn5+WmhoaFakyZNtLffflt1K7dKS0tT3c9Lly6t+fr6atHR0dqoUaMc9hHSDf+BBx74zy7kWXXlF2vWrNGqVq2qzqdixYra//73P6eu/OvWrVNDERQtWlTtJz87deqkrifjc2Ts7v7DDz+oawwMDNTCwsK0du3aaXv37nXYx/p8GYcKkGPJejl2drvyZyWrrvwy5EGRIkXU+cl5bt26NdMu+F9//bVWpUoVzcfHx+E6Zb+77ror0+e0P05CQoJ6vWrXrq1eX3tDhgxRwxvIcxNR9pnkfzkJpoiIiIj0jDVHRERERHYYHBERERHZYXBEREREZIfBEREREZEdBkdEREREdhgcEREREdnhIJAGI9NHnDp1Sg0458qpC4iIyP1k9B2ZvkbmVZRBSd0lOTlZDSbrCjJ5ckBAADwJgyODkcAoOjo6r0+DiIhyITY2Vo0k767AqHTJEJw555rJn2WqnSNHjnhUgMTgyGAkYySWbSmFoBBjtaq++e+cXYZicd3M9p7E5OsHI9LSXPNN35MY7bVO19KwKX257W+5O6SmpqrA6Mj2kggLzd3nRMIVC0rXOaaOyeCI7ljWpjQJjIJz+ab3ND4mXxiOyVivsZXJiK+1BEcm4014YNTX+naURYSFeuU6OPJUDI6IiIjIiVmzwKzl/hieiMERERERObFAU0tuj+GJjJkvIyIiIsoCM0dERETkxKL+y/0xPBGDIyIiInJi1jS15PYYnojBERERETmxsOaIiIiIiAQzR0RERJRp1sds0MwRgyMiIiJyYmGzGhEREREJZo6IiIjIiZm91YiIiIhukBGKcj/OkWdisxoRERGRHWaOiIiIyInZBb3Vcvv4vMLgiIiIiJyYtetLbo/hidisRkRERGSHmSMiIiJyYjFwQTaDIyIiInJigQlmmHJ9DE/E4IiIiIicWLTrS26P4YlYc0RERERkh5kjIiIicmJ2QbNabh+fVxgcERERkRMzgyOiW3P812Bsfa8QzvwZhKvnfPHovCOoeP9l23aZVmfjzCjs/DQSKQneKF4nEW1eiUVE6VTbPrPvroLLJ/0cjtti+Ck07ncOnqpqgyt4rO9ZlK+WhMioNIzvWQZbV+eDEbTrdgGP9juHiILpOLw3EHNGF8P+XUHQqyeeO4UmreNQvGwyUpO9sHd7CD56rThOHA6E3vG1Ns5rbTSsObpDHT16FCaTCbt27cKdLPWaFwpXTkLMhBOZbt/6biH8tqAg2kyKRbcvD8A3yIJPupVFeorjt4l7hpzGoG1/2pa6XS/AkwUEWXB4bxBmj46GkTR7KA59xp3C4ulR6B9TAYf3BmDyksMIj0yDXlVrcAUrFxbGkA5VMOrpivDx1TB50QH4B5qhZ3yt9f9aWzSTSxZPZJjgqFu3birYsC6RkZFo3bo1/vjjD5c9R6lSpTBz5kyXHCs6OhqnT59G1apVcScr1/wKmr9wBpVibmSL7LNGv84viKYDzqDifQkoXDkZD715DFfO+mL/mnCHff2DLQgpmG5b/II8dXSM637/KRwfTy2KLd8bI1tk1bHPBXy/JAJrlkbg+MEAzBpZHClJJsR0ugS9Gt21ItZ+XgDHDgbiyN9BmPZCaRQunory1a5Bz/ha6/+1Nv/brJbbxRMZJjgSEgxJwCHLunXr4OPjgwcffBB3mtTUVHh7eyMqKkqdo6eKj/VD4nlflGpy1bYuIMyCYjWv4eTOYId9t8wrhOm1q+KDBytg63sFYUnPgxOmXPHxtaB89WvYsSnUtk7TTNi5KRRV6ujzwyMzQaHXswhX4r2hV3ytjfNaG5WhgiN/f38VcMhSs2ZNvPjii4iNjcX58+fV9pEjR6JChQoICgpCmTJlMGbMGKSlOaaIV65ciXr16iEgIAAFChTAww8/rNY3b94cx44dw5AhQ2zZKavNmzfj7rvvRmBgoMoIDRw4EImJiQ4Zp1deeQVdunRBWFgY+vTp49SsZjab0bNnT5QuXVodp2LFinjrrbdwJ0s8fz2wCy7g+DuU+1f/3SbqdT2Ph2cdQ+fFh1Cr00VsmVMY614retvPl3InLMIMbx8g3u61FXEXfJC/oDGiXZNJQ99xx/HXbyE4dkC/tTd8rY3xWpvh5ZLFE3nmWbvA1atX8b///Q/lypVTTWwiNDQUCxYswN69e1Xg8f7772PGjBm2x3z77bcqGGrbti127typsk/169dX27788ksUL14cEydOtGWnxD///KMyVo888ohqwlu6dKkKlgYMGOBwPm+++SZq1KihjitBWUYWi0Ud/7PPPlPnN3bsWLz00ktYtmzZTa8zJSUFCQkJDsudpkGv8yjZ8KpqdqvT+SJavnQKvy8s6FSXRHSn6//KMZSqkIQpA8rm9amQmxnhtdZcUG8kx/BEnttmcwu++eYbhISEqNuSuSlSpIha5+V1PUYcPXq0QzZn2LBh+PTTTzFixAi1bvLkyXjyyScxYcIE234S0IiIiAjVFCYBlmSmrKZMmYLOnTtj8ODB6n758uUxa9YsNGvWDHPnzlUZKHHvvffihRdesD1OMkf2fH19HZ5XMkhbt25VwdHjjz+e5TXL89s/7nYK/vcbZOIFX4QWuvFtUu4XrpKU5eOk2c2SblI92CLLpNyWc6XcS7jkDXM6kC9D5iB/gXTEZcgw6NFzE4+hQct4DHu8Mi6ccex9qTd8rY3zWhuVoTJHLVq0UM1Usvz666+IiYlBmzZtVHOYkKxOkyZNVHAjQZQES8ePH7c9Xh7XsmXLHD3n7t27VTZKjmdd5HklE3TkyBHbfnXr1v3PY73zzjuoU6cOChYsqI7z3nvvOZxfZkaNGoXLly/bFmlGvF3yRaciuGAajm65HpCKlCteOLkrCMVq3WhWzOjs3kCYvDQERRojPa8X6WleOPhHEGo1veLQ9FCz6VXs3a7PZofrNPVh2TgmDiM7VcLZWH/oHV9rY7zWZgMXZOs/xLcTHBysmtGsPvjgA4SHh6vmswceeEBleCTLIsGLrJes0bRp02z7S63PrTTfPfvss6rOKKMSJUo4nNvNyLlIJkvOp1GjRipDNXXqVGzbtu0/66xkcZfURC9cOubvUIR9Zm8gAsPTEV4sDfW7n8fPswsjolQK8hVPxYYZRRBaOM02FtKJHUE4tSsYJRtdgV+wBSd3BGPt5KKo2iEOgeGe2z02IMiMoqVuZL2iolNQpso1XIn3wflT+v2m+eV7BTBsZiwO7A7C/p1BeLj3eTWswZpPI6BX/ScdQ4uHLmFC73JISvRG/oLXa+wSE7yRmqLf7598rfX/Wps1L7Xk7hjwSIYKjjKSgmdpUktKSsKWLVtQsmRJvPzyy7bt1oySVfXq1VWdUffu3TM9np+fnyqctle7dm1VI2QflN2Kn3/+GY0bN8Zzzz1nWyf1THnt9J4g/O+pG9f2w+Ri6mf1Ry6h3dTjaPTsOaQleWHVS9FITvBGdN1EPDn/MHz8r/+L8fbT8Nc3+bDxrSiYU00q2yQBVYOe14vkPVWFGtcw9bODtvt9x59UP9csi8C0oaWgVxtW5Ed4pBldhp9RhbmH/wrEy51LI/6CL/Sq3TPX36tTl+13WC/dvKXbt17xtdb/a22BCZZcNjBZ4JnRkaGCIylOPnPmjLodFxeH2bNnq8xOu3btVKGyNFFJhkZ6o0nx9VdffeXw+HHjxqlmtbJly6rao/T0dKxatUr1crPWKW3cuFFtk2yN9GaTbQ0bNlQF2L169VIZIgmW1q5dq54/u6RWaeHChVi9erWqN1q0aBF+++03dTsvSSH1y4ezHqhSOu01G3JGLZkpUjUJ3b+8EUToxR9bQxFTvDaMaMX8AmoxitYl68Go+FqTXukvD3gT33//vSrClqVBgwYquJDeX9IN/6GHHlLd8CWIkW7+kknK2GtM9pP9V6xYofaRImqpXbKSnmpSSC3Bk9QFWbNNGzZswIEDB1R3/lq1aqmeZkWL5qyrujTNdezYEU888YQ694sXLzpkkYiIiFzJbOCaI5OmyTjGZBSSIZN6qm/+KIPgUEPFxphcrg4Mx+K5dVu5YfLVb13XzWhpN+YsNAqjvdbpWhp+SvtMdbCRcfHc+Tnx1e7yCA7N3QCXiVfMeLjGQbeerzsY69ORiIiI6D8YquaIiIiIclKQbcr1MTwRgyMiIiJyYnHB9B+e2luNzWpEREREdpg5IiIiIjcNAqnBEzE4IiIiokyb1SxsViMiIiIiZo6IiIjIiVkzqSW3x/BEDI6IiIjIidkFvdXMbFYjIiIivbBoXi5ZckLmJ5X5TmWKLZkcfvny5Q7bZVIPmYJLpgELDAxEq1atcPCg4/ycly5dQufOndWI3Pny5UPPnj3VPKo5weCIiIiI7giJiYmoUaMG3nnnnUy3v/HGG5g1axbmzZuHbdu2qcncY2JikJycbNtHAqO//vpLTfD+zTffqICrT58+OToPNqsRERHRHdGs1qZNG7VkRrJGM2fOxOjRo9G+fXu1buHChShcuLDKMD355JP4+++/1STzMrF83bp11T5vv/022rZtizfffDPbk74zc0REREROLHZF2be6yDGsk9naLykpKTk+nyNHjuDMmTOqKc1KJsht0KABtm7dqu7LT2lKswZGQvb38vJSmabsYnBEREREbhUdHa0CGesyZcqUHB9DAiMhmSJ7ct+6TX4WKlTIYbuPjw8iIiJs+2QHm9WIiIjITYNAeqmfsbGxqkDayt/fH3cyBkdERETkpulDvNRPCYzsg6NbERUVpX6ePXtW9Vazkvs1a9a07XPu3DmHx6Wnp6sebNbHZweb1YiIiOiOV7p0aRXgrFu3zrZO6peklqhRo0bqvvyMj4/H9u3bbfv8+OOPsFgsqjYpu5g5IiIiIicWSEF17ka4zunjZTyiQ4cOORRh79q1S9UMlShRAoMHD8akSZNQvnx5FSyNGTNG9UDr0KGD2r9y5cpo3bo1evfurbr7p6WlYcCAAaonW3Z7qgkGR0REROTWZrXs+v3339GiRQvb/aFDh6qfXbt2xYIFCzBixAg1FpKMWyQZoqZNm6qu+wEBAbbHLF68WAVELVu2VL3UHnnkETU2Uk4wOCIiIiI3jXPklaP9mzdvrsYzyoqMmj1x4kS1ZEWyTEuWLEFusOaIiIiIyA4zR0REROTEIoM4armsOcrl4/MKgyMiIiLKdIwis4vGOfI0DI4M6s3q1eBj8oWRrD51o2unUcQUvT72h9F4lY6GIV2Mg9GYL16CkWhaWl6fgiEwOCIiIiInFs1LLbk9hidicEREREROzDCpJbfH8ESeGdIRERERuQkzR0REROTEwmY1IiIiohvMLmgWk2N4Is8M6YiIiIjchJkjIiIicmJhsxoRERHRDXkx8eydgsEREREROdFggiWXNUdyDE/kmSEdERERkZswc0REREROzGxWIyIiIrrBopnUkttjeCLPDOmIiIiI3ISZIyIiInJihpdacnsMT8TgiIiIiJxY2KxGRERERIKZIyIiInJigZdacnsMT8TgiIiIiJyYNZNacnsMT+SZIR0RERGRmzBzRERERE4sBi7IZnBERERETjTNC5ZcjnAtx/BEDI6IiIjIiRkmteT2GJ7IM0M6IiIiIjdh5oiIiIicWLTc1wzJMTwRgyMiIiJyYnFBzVFuH59XGBx5sKNHj6J06dLYuXMnatasiTtZu24X8Gi/c4gomI7DewMxZ3Qx7N8VBE+055dgfDanEA7uCcKls74Y9+ERNG5z2bZ986pwfLswUm2/EueDOWv2o2zVJKfj7P09CAteL4J9O4Lg7Q2UuSsJry75B/6BHvpVS4evdWaqVr+AR548gHIV4hFZIBmvjG6IrZuL2rZ37rYX99x7AgULJiEt3QuHDuTDwg/uwv6/I6AnkYVS0H3IP6jb9CL8Ayw4HRuIGaMr4eDeMOid3t/jxJqjXOnWrRtMJpPTcujQodvy/NHR0Th9+jSqVq2KO1mzh+LQZ9wpLJ4ehf4xFXB4bwAmLzmM8Mg0eKLka14qkBnw6okst99VPxE9XzqV5TEkMHq5c1nUuecKZq06iFmrDuCh7hdg8vB/kXp7rTMTEJCOI/+EY87MGpluPxkbirlv1cRzPVph+PPNcO5MECZN3Yyw8BToRUhYGt5cuAPmdBPG9quBvh3q4/2p5XAlwRd6Z4T3uJUFJpcsnoiZo1xq3bo15s+f77CuYMGCt+W5vb29ERUVhTtdxz4X8P2SCKxZev2b86yRxVG/ZQJiOl3CstmF4Wnq3XtFLVlp9Wic+nkm1i/Lfd4dXwwdep7HE8+fs62LLuf5H556e60z8/uvUWrJyvp10Q7333unOmIeOIbSZS9j945C0INHexzH+TP+mDGmsm3d2ZOBMAIjvMetOEI23TJ/f38VoNgvPXv2RIcOHRz2Gzx4MJo3b267L7cHDhyIESNGICIiQj1u/PjxDo/Zt28fmjZtioCAAFSpUgU//PCDykwtX77c1qwm93ft2oU7lY+vBeWrX8OOTaG2dZpmws5NoahS5xqMKP6CD/btCEa+yHQMblceT1S/C8M6lsOf24LhyfhaO/PxsaBNuyO4etVXZZv0omHzCzi4NxSjpv2JJes34+1lvyHmkawzpXrB97hxMHOUhz7++GMMHToU27Ztw9atW1UzXZMmTXDffffBbDarAKtEiRJq+5UrV/DCCy/k+DlSUlLUYpWQkIDbKSzCDG8fIP6841st7oKPLjIlt+L0sesZpUXTo9B7zCmUvSsJP3yeHy8+URbv/rgPxcqkwhPxtb6hfqPTGDn2V/j7m3HpYgBefqEJEi77Qy+iiifjgcdP4auFxbH0/ZKoUPUK+r54EOlpJqxbUQR6ZbT3uIUF2XSrvvnmG4SEhNjut2nTBsHB2csAVK9eHePGjVO3y5cvj9mzZ2PdunUqOFq7di3++ecfrF+/3tZ0NnnyZLUtJ6ZMmYIJEybk6DHkXhbL9Z9tn76ImCcvqdvlqiVh1+ZQrP40Ej1eOp23J0i5tntnQQzo1RJh4alo/cARjBr/K4b0a47L8QHQA5OXhoN/heLjWWXV/cP7QlGy3FW0ffyUroMjo7FIzVBuu/J7aM2RZ4Z0d5AWLVqoZi3rMmvWrGw/VoIje0WKFMG5c9drUPbv368Kru1riurXr5/j8xs1ahQuX75sW2JjY3E7JVzyhjkdyFcw3WF9/gLpiMvw7csoIgtf/12UrJDssD66XDLOnfTcgla+1jekJPvg9MkQ7N8bgbem1oHZbEJM22PQi7jzfoj9x/FLYOzhYBSMcnxP6w3f48bB4CiXJEtUrlw52yIBjpeXFzTNsTt2WppzTwZfX8cPQqkfsljTCi6siQoLC3NYbqf0NC8c/CMItZreKGA2mTTUbHoVe7cbs+tr4ehUREal4sQ/js0sJw/7o1Bxz+3xwtc6a14mwNfPDL3YuyscxUo51tjI/XOn9ZEZy4rR3uOaC3qqyTE8EUNdN5Dean/++afDOskqZQyGbqZixYoqy3P27FkULny9B8Rvv/0GT/TlewUwbGYsDuwOwv6dQXi493kEBFmw5lPPHPclKdELp47cCGykV9o/fwYiNF+6Cm4S4rxx/qQfLp69/s8r9t8gKH+hNEQUSofJBDza7zwWvRmFMlWS1LAAP3wWgdh/AjD6/aPwZHp7rTMTEJiOosWu2u4XjkpEmXLxuJLgh4QEPzz59D78sqUo4i4GqO77D3Y4jMiCSdi0vjj04quF0Zi2aAce73UUm1YXQsVqV9DmkVOYNbEi9M4I73EraVLL/QjZDI7oX/feey+mTp2KhQsXolGjRvjf//6ngqVatWpl+xhSW1S2bFl07doVb7zxhirIHj16tC3D5Ek2rMiP8Egzugw/g/wyaNpfgXi5c2nEX/DMJiT5ozji0XIO3fLFfY9fwrCZx/HLmnBMG1LCtn1Kv1Lq59NDz+CZYWfU7Y69zyMt2YR544rhSrw3ylRJxpRP/kHRUp5ZjK3X1zoz5SvG4fWZm2z3+wzYo36u/b4EZk+vheIlruLlmF8QHp6qgqUD+/Jj+PP34PhR/QyOePCvMEwaXBXdBh/GU32P4czJALz7Rnms//bOH1okt4zwHrdiQTa5VExMDMaMGaO66ScnJ6NHjx7o0qUL9uy5/kc0u2MYSZf9Xr16oV69eihTpowKuNq1a6e69nuaFfMLqEUPajS+itWnsh4+4f4nLqnlv8gYR/bjHOmFnl7rzOzZVRBtm3fMcvvksQ1hBL9uLKAWI9L7e5wYHOXKggULstwmPcRu1ktMeqFlZB2/yKpSpUrYvHmz7f7PP/+sfkptkyhVqpRTbRMREZErWNisRneir776Sg0TIN38ZUqSQYMGqXGQpLmNiIjInSwumP7DU7vyMzi6g0md0ciRI3H8+HEUKFAArVq1wrRp0/L6tIiIiHSNwdEdTOqUZCEiIrrdLGxWIyIiIrrByMGRZ/axIyIiInITZo6IiIjIicXAmSMGR0REROTEYuDgiM1qRERERHaYOSIiIiInmgvGKfLUYYoZHBEREZETi4Gb1RgcERERkROLgYMj1hwRERER2WFwRERERFlmjiy5XLLLbDZjzJgxKF26NAIDA9U8oq+88orDBOtye+zYsShSpIjaR6bVOnjwoMuvncERERER5Xlw9Prrr2Pu3LmYPXs2/v77b3X/jTfewNtvv23bR+7PmjUL8+bNw7Zt2xAcHIyYmBgkJye79NpZc0RERERulZCQ4HDf399fLfa2bNmC9u3b44EHHlD3S5UqhU8++QS//vqrLWs0c+ZMjB49Wu0nFi5ciMKFC2P58uV48sknXXa+zBwRERGRE00zuWQR0dHRCA8Pty1Tpkxxer7GjRtj3bp1OHDggLq/e/dubN68GW3atFH3jxw5gjNnzqimNCs5VoMGDbB161aXXjszR0REROTEAlOuxzmyPj42NhZhYWG29RmzRuLFF19UGaZKlSrB29tb1SBNnjwZnTt3VtslMBKSKbIn963bXIXBEREREblVWFiYQ3CUmWXLlmHx4sVYsmQJ7rrrLuzatQuDBw9G0aJF0bVrV9xODI6IiIgoz8c5Gj58uMoeWWuHqlWrhmPHjqkmOAmOoqKi1PqzZ8+q3mpWcr9mzZpwJdYcERERkVtrjrLj2rVr8PJyDEukec1isajb0sVfAiSpS7KSZjjptdaoUSO4EjNHRERElOfatWunaoxKlCihmtV27tyJ6dOno0ePHmq7yWRSzWyTJk1C+fLlVbAk4yJJs1uHDh1cei4MjoiIiCjPm9XefvttFew899xzOHfunAp6nn32WTXoo9WIESOQmJiIPn36ID4+Hk2bNsX333+PgIAAuBKDIyIiInKi5bBZLKtjZFdoaKgax0iWrEj2aOLEiWpxJwZHZBgxRV1bsOcJLM1qwYi8dx+GEZnjL8NovPOFw0g0LRWIv13PZcp15ii3wVVeYUE2ERERkR1mjoiIiMiJpjI/uT+GJ2JwRERERJmObm1y0QjZnobNakRERER2mDkiIiKiPO+tdidhcEREREROLJoJpts4ztGdhM1qRERERHaYOSIiIiInmuaC3moe2l2NwRERERE50Qxcc8RmNSIiIiI7zBwRERGRE83AmSMGR0REROTEYuDeagyOiIiIyIlm4IJs1hwRERER2WHmiIiIiLLIHJlyfQxPxOCIiIiInGgGLshmsxoRERGRHWaOiIiIyIn275LbY3giBkdERETkRGOzGhEREREJZo6IiIjImWbcdjUGR0RERORMy32zmhzDE7FZjYiIiMgOM0dERETkRDPw9CEMjoiIiMiJZuDeagyO6LZo1+0CHu13DhEF03F4byDmjC6G/buCoHd6vu4n2/+BpvWOIbroZaSk+mDvgYL44JO6OHE6XG0PDU5Bl8d2ok61UyhUIBGXEwLw8+8lsGBZLVxL8oNedO5/FJ37H3dYF3s4EM8+WA96p+f3d2YM91prptzXDDE4osysX78eLVq0QFxcHPLly5flfqVKlcLgwYPVojfNHopDn3Gn8PaLxbFvRxAe7n0ek5ccRs+7K+LyRV/old6vu3rlM1ixphL2Hy4Aby8NPZ7cgddGrUGv4R2QnOKLyPzXEJkvCe8trodjJ8JRuGAiBvXcqta/MrMF9OTowSC83LO67b453TM/EHJC7+/vrBjxtTYiwxZkd+vWDR06dMg0mDGZTIiPj3fL8y5YsOCmQZIedexzAd8vicCapRE4fjAAs0YWR0qSCTGdLkHP9H7dL712P9ZsLI9jJ/Lj8PEITJ3bVAVA5UtfVNuPnsiPiTNb4Jcd0Th9Lgy7/iqC+Utro2HtWHh5WaAnZrMJcRf8bEtCvH6DA6O8v7NipNda01yzeCJmjsitfHwtKF/9Gj6dXcihDXrnplBUqXMNemXE6w4OSlU/r1z1v+k+15J8YbHo63tZsRJJWLT+F6SmeGHf7lAsmFEa508HQK+M+P425GutGXecI339hXKDzZs34+6770ZgYCCio6MxcOBAJCYm2rYvWrQIdevWRWhoKKKiovDUU0/h3LlzmR5LslLdu3fH5cuXVXZKlvHjx9u2X7t2DT169FDHKlGiBN577z3btnvvvRcDBgxwON758+fh5+eHdevW4U4VFmGGtw8Qf94xDo+74IP8BdOhV0a7bpNJQ78uv+LPfYVUxigzYaHJ6PzwbqxaVxF6sv+PMEx/uSLG9KmKdyaWQ+FiKZi6aDcCg/T3Ohv1/W3k19qospU5WrFiRbYP+NBDD0Ev/vnnH7Ru3RqTJk3CRx99pIIRCVBkmT9/vtonLS0Nr7zyCipWrKiCoqFDh6omu1WrVjkdr3Hjxpg5cybGjh2L/fv3q3UhISG27dOmTVPHeumll/D555+jX79+aNasmTp2r1691PPKPv7+17+Z/+9//0OxYsVU4JSVlJQUtVglJCS49HdEJJ7v/gtKRcdhyPi2mW4PCkzFpBE/4NjJfFj4RU3oye+bImy3jx64/gG64IdtuLv1eaz5skienhu5ltFea4291W4us9qczEgmxGw2w1N88803DsGJsD//KVOmoHPnzrYi6fLly2PWrFkqYJk7dy4CAgJUpseqTJkyanu9evVw9epVp2NLlic8PFz9niTLlFHbtm3x3HPPqdsjR47EjBkz8NNPP6ngqGPHjio4+vrrr/H444/b6pckEJPjZUWuYcKECcgrCZe8YU4H8mX4Npm/QDriMnzr1BMjXfeAbr+gQe1YvDChDS5cCnbaHhiQhldfXIukJF+Mn94CZrO+E9aJV3xw8mggipZMhl4Z6f1t9NcaHtosllvZ+itlsViytXhSYCSkF9muXbsclg8++MC2fffu3SoAkSDHusTExKhrPXLkiNpn+/btaNeunWoGk+YwCZzE8eOO3T2zo3r1Gz0grAGUtYlOArFnnnlGZbDEjh078Oeff6rg6GZGjRqlmvGsS2xsLG6n9DQvHPwjCLWaXnFogqnZ9Cr2btdvl19jXLemAqMm9Y5jxKTWOHM+NNOMkfRgS0/3wtg3WyItTf8fnAFBZhQpkYxL5/UzXIEx39//zQivtVHl6i9VcnKy+tD2VMHBwShXrpzDuhMnTthuS/bn2WefVXVGGUkwJLVHEizJsnjxYhQsWFAFRXI/NfV6cWpO+Po69nqQAEkCMStpWqtZs6Y6R2nWk+a0kiVL3vSY0gRnbYbLK1++VwDDZsbiwO4g7N95vctvQJAFaz69kaLWI71f9/M9fsG9jQ9j3LSWuJbkg/zh1wtxE6/5ITXNxxYY+fub8dq0Fuq+LELGPLJo+sgg9Rx+GNt+isC5UwGILJSCpwccg8VswvpvC0LP9P7+zozRXmuNzWrZJ9mhV199FfPmzcPZs2dx4MAB1Zw0ZswYNVZPz549oRe1a9fG3r17nQIoqz179uDixYt47bXXVLG2+P333296TGlau9UMW7Vq1VTx9/vvv48lS5Zg9uzZ8AQbVuRHeKQZXYafUcWah/8KxMudSyP+gn67wBrhuh+673rd3LSx3zusnzq3ieriX67URVQuf0GtW/jWlw77PP38Izh7wTnT5IkKFE7ByDf3ISxfGi5f8sVfO8IxpFNNJMTpO5ug9/d3Zgz3WmvG7a2W4+Bo8uTJ+Pjjj/HGG2+gd+/etvVVq1ZVxcZ6Co6k7qdhw4aq1keyNpJpkmBp7dq1KjCR7JEEO2+//Tb69u2rmrmkoPpmJICUjJT0MKtRowaCgoLUkl3Wwmw5l4cffhieYsX8AmoxGj1f932dbt6k+8ffRf5zHz14fVhlGJWe39+ZMfJrbTQ5zmsvXLhQdTGXQmVvb2/bevmg37dvH/REaoA2bNigsmPSnb9WrVqqp1nRokXVdmlGk5qkzz77DFWqVFEZpDfffPOmx5QeaxJIPfHEE+rxEmTmRKdOneDj46N+enKTJhER3elMLlo8j0nTcjZ+pYz3I0GQ1LpIAbIULUuzmmRU6tevr7Ii5D5Hjx5F2bJl8dtvv6lmv5ySrvzSY6452sPHpN/0N11naVYLRuS7+zCMyBx/GUbjne/6XH5Gka6lYl38ItXBJiwszC3PkfDv50T03PHwCszdl3BLUjJi+4136/neEZkjyZBs2rTJab2MyyOZFXIPGU/pzJkzGD16tGrqu5XAiIiIKMc1R1ouFyPUHEmzUteuXXHy5EnVk+rLL79UAxpKc5uMG0Tu8fPPP6uhBypUqKACUSIiIrpDgqP27dtj5cqVmDhxoioKlmBJshiy7r777nPPWRKaN2+OHLaAEhER3TrNdH3J7TGMMs6RFCdLjy0iIiLSJ027vuT2GIYaBFLG8/n7779tdUh16tRx5XkREREReUZwJKMzSzdyqYHJly+fWhcfH6+6qH/66acoXry4O86TiIiIbifNuINA5ri3mgxCKD2nJGt06dIltchtKc6WbURERKSjmiMtl4sRMkcyKOKWLVvUTPFWcltGiZZaJCIiIiJPluPgSOYQk8xRRjJfmHXkaCIiIvJsJu36kttjGKJZberUqXj++ecdJliV24MGDfrPqTOIiIjIQ2gcBPKm8ufPD5PpRrthYmIiGjRooOb4Eunp6ep2jx490KFDB/edLREREdGdEBzNnDnT3edBREREdxKNg0DelEwXQkRERAaisSv/LUlOTlaz99ovREREpAPa7a85knlbn376aURGRiIwMBDVqlVzqHGWabRk2rIiRYqo7a1atcLBgwfzPjiSeqMBAwagUKFCam41qUeyX4iIiIhyKi4uDk2aNIGvry++++477N27F9OmTXOILd544w3MmjUL8+bNw7Zt21QcEhMTo5I1edqVf8SIEfjpp58wd+5cPPPMM3jnnXdUpPfuu+/itddec+nJERERkTGa1V5//XU1XND8+fNt60qXLn3jUJqmaqBHjx6N9u3bq3ULFy5E4cKFsXz5cjz55JPIs8zRypUrMWfOHDzyyCOqh5oM/Cgn+uqrr2Lx4sUuOzEiIiLSxwjZCRlKcFJSUpyebsWKFahbty4ee+wx1TpVq1YtvP/++7btR44cwZkzZ1RTmlV4eLjqPb9161aXXnqOgyOZLqRMmTLqdlhYmLovmjZtio0bN7r05IiIiMjzRUdHq0DGukyZMsVpn8OHD6tWqfLly2P16tXo168fBg4ciI8//lhtl8BISKbInty3bsuzZjUJjCR6K1GiBCpVqoRly5ahfv36KqNknYiWiIiIPJvJhSNkx8bGqoSKlb+/v9O+MkerZI6kJUpI5ujPP/9U9UW3u9d8jjNH3bt3x+7du9XtF198UdUcBQQEYMiQIRg+fLg7zpGIiIg8uLdaWFiYw5JZcCQ90KpUqeKwrnLlyjh+/Li6HRUVpX6ePXvWYR+5b92WZ5kjCYKspN1v37592L59O8qVK4fq1au79OSIiIjIGJo0aYL9+/c7rDtw4ABKlixpK86WIGjdunWoWbOmWif1S9JrTZrg8jQ4ykhO2nriRERERLdCki+NGzdWzWqPP/44fv31V7z33ntqETKN2eDBgzFp0iRVlyTB0pgxY9Sk966euixbwZGMKZBdUjxFREREns1kVzOUm2NkV7169fDVV19h1KhRmDhxogp+pOt+586dHYYTkvEW+/Tpg/j4eNUZ7Pvvv1flPa5k0mTggP9gP87ATQ9mMqlqc7pzSQpSego0R3v4mHxhJD6ljZfhNJ84DSOKWB8MI7p4N2cp0Lt0LQ3rLV/i8uXLDgXO7vicKPn6JHjlMuiwJCfj2MjRbj1fd8hW5kh6pxEREZGBaJx4loiIiOgGA088y+CIiIiInGnGDY5yPM4RERERkZ4xc0RERERuHSHb0zA4IiIiImcam9VyZNOmTXj66afRqFEjnDx5Uq1btGgRNm/e7OrzIyIiIrqzg6MvvvgCMTExCAwMxM6dO5GSkqLWyxgG1sniiIiIyMNprptbTffBkQzbLTPkvv/++/D19XWYE2XHjh2uPj8iIiLKw5ojUy4XQwRHMincPffc47ReRtOUobyJiIiIPFmOgyOZEffQoUNO66XeqEyZMq46LyIiIroTRsjWcrkYITjq3bs3Bg0ahG3btqm51E6dOoXFixdj2LBh6Nevn3vOkoiIiG4vzbg1Rznuyv/iiy/CYrGgZcuWuHbtmmpi8/f3V8HR888/756zJCIiIrpTgyPJFr388ssYPny4al67evUqqlSpgpCQEPecIREREd12Jg4CmXN+fn4qKCIiIiId0ow7CGSOg6MWLVqo7FFWfvzxx9yeExEREeU1zQWZH6MERzVr1nS4n5aWhl27duHPP/9E165dXXluRERERHd+cDRjxoxM148fP17VHxEREZEOaMZtVruludUyI3OtffTRR646HBEREeUlzbhd+V0WHG3duhUBAQGuOhwRERGRZzSrdezY0eG+pmk4ffo0fv/9d4wZM8aV50ZERER5xMSu/Nknc6jZ8/LyQsWKFTFx4kTcf//9rjw3IiIiojs7ODKbzejevTuqVauG/Pnzu++siIiIiDyh5sjb21tlh+Lj4913RkRERJT3NBZkZ1vVqlVx+PBh95wNERER3VE1R6ZcLoaoOZo0aZKaZPaVV15BnTp1EBwc7LA9LCzMledHOtGu2wU82u8cIgqm4/DeQMwZXQz7dwVBL+6qcRGPPHUI5SrFI7JACl55sR5+2VTEtr1xs1No0+EYylWMR1h4Gp7v1gyHDzrW73m6J547hSat41C8bDJSk72wd3sIPnqtOE4cDoReaGYNSR8mI2VNKiwXLfAq4AX/tn4I7BbgMHNA+lEzrs1JQvquNGhmwLuUN0Inh8A7ymUdhPNc1QZX8FjfsyhfLQmRUWkY37MMtq7OB70z6nUbTbb/pUrBdWJiItq2bYvdu3fjoYceQvHixVXtkSz58uVjHVIG3bp1U38wZZG56MqVK6d+j+np6bk67vr169UxPaV5s9lDcegz7hQWT49C/5gKOLw3AJOXHEZ4ZBr0IiAwHUcOhWHutOqZbvcPMGPvHxGYP1e/8xFWa3AFKxcWxpAOVTDq6Yrw8dUwedEB+AeaoRdJ/0tG8vIUBA8NQr4lYQh6LhBJi5OR/HmKbR/zCTMS+l2Bd0kvhM0ORb6PwxAkwZM/dCUgyILDe4Mwe3Q0jMRw160Zr0ktR5mjCRMmoG/fvvjpp5/ce0Y607p1a8yfPx8pKSlYtWoV+vfvD19fX4waNQpG0bHPBXy/JAJrlkao+7NGFkf9lgmI6XQJy2YXhh5s/6WwWrLy0+rrf0gLRV2DXo3uWtHh/rQXSmPpzl0oX+0a/vw1FHqQ/qcZfnf7wq+xr7rvXcQbqWtTkb73RgB47b0k+DbyRXD/G5lR7+Le0JvffwpXi9EY6ro1jpD9n2Q8I9GsWbObLuTI398fUVFRKFmyJPr164dWrVphxYoVKliS5slixYqppskGDRqojJDVsWPH0K5dO5WNk+133XWXCq6OHj2qJv8Vsk0ySJKhulP5+FpQvvo17Nh048NR00zYuSkUVeroN1AgICj0esBwJV4/gYFPVW+k/Z4O8/Hr15Z+MB1pf6TDr+H175maRUPqljR4R3shYcgVXHogHpd7JyB1Y2oenzlRzplYc5Q99m3qdGsCAwNx8eJFDBgwAHv37sWnn36KokWL4quvvlJZpj179qB8+fIqw5SamoqNGzeq4Ej2DQkJQXR0NL744gs88sgj2L9/v6rxkmNmRYIwWawSEhJwO4VFmOHtA8Sfd3yrxV3wQXS5G+dF+mIyaeg77jj++i0Exw7op7Ys8JkAaNc0xD+VcP2rpQUI6hMA/5jrbWZanAYkXW9+C+odiKB+gUjblo4rLyUi7G0TfGtdzzgRkY6CowoVKvxngHTp0qXcnpMuSeZt3bp1WL16NTp16qSa2o4fP64CIyFZpO+//16tf/XVV9U2CYBkTClRpkwZ27EiIq43TxUqVEjVet3MlClTVJMo0e3U/5VjKFUhCS88Whl6kvpjGlLXpCJkfDC8S3vDfDAdiW8lwVTACwFt/VWwJKTpLfDJ69Mp+VTwQdqedFWrxOCIPIpm3Ga1HAVH8iGbcYRsurlvvvlGZXzS0tJgsVjw1FNP4dFHH8WCBQtUsGlPMjyRkZHq9sCBA1Uz3Jo1a1RTnARK1atnXux7M1LbNHToUIfMkWSfbpeES94wpwP5CjoWoecvkI64DNkk0ofnJh5Dg5bxGPZ4ZVw44wc9ufbONQQ+HQD/Vtevy6esN8xnLEhalKyCI1M+E+B9vXeaPe9SXkj/I3cdMYhuNxOnD8meJ598UmUrKPukPmju3Lmqt5pkiXx8fLB06VI1oOb27dvVT3sSSIlevXohJiYG3377rQqQJAM0bdo0PP/88zmueZIlr6SneeHgH0Go1fQKtn4fbmtyqdn0KlYsuB4Ikl5oeG7icTSOicOIJyrhbKy//q4wWSo1HbPnJmle+/cDwORrgk9lb5iP/5tC+pc51gIvHXXjJ9K7bAdHrDe6NVIvJF347dWqVUtNxXLu3DncfffdWT5WMjzSQ1AWyQC9//77KjiSQEvIMTzBl+8VwLCZsTiwOwj7dwbh4d7nVXfYNZ9ebx7US1f+osUTbfejil5DmfKXcSXBF+fPBiEkNBWFopIQUUA+XYFiJa6qn3EX/RF36Xrzi6frP+kYWjx0CRN6l0NSojfyF7w+VENigjdSU/QRGPg18UXSx0nwKuwF79JeSD9gRtLSFPg/cCNDFvBUAK6OTURyTR/41vZB6i9pSPs5DWFv66PHnlVAkBlFS92oG4yKTkGZKtdwJd4H50/pK2No2OvW2KyW7d5qlHvSnNa5c2d06dJFZYMkWDp//ryqSZKmswceeACDBw9GmzZt1L5xcXFqCIXKla/Xb0jPNwlWpclOxp2SgmxrxulOtGFFfoRHmtFl+Bnkl0Eg/wrEy51LI/6CfuovyleKx2uzt9ju9x74l/r5w6pozJhcCw3vPoMhL++ybX9x4nb1c/GHFbDko0rQg3bPnFc/py7b79Slf+3nBaAHwUOCcO39JCS+eQ2WuOuDQAa090dg9xsBrn8zP2jDNdXUljjDAu8SMgBkMHxr6KsZuUKNa5j62UHb/b7jT6qfa5ZFYNrQUtArQ123ZtzgyKQx6nEb6WIvAzUuX77caZvUIMlo4wsXLsTJkydRoEABNGzYUNV1SRG2ZIi+++47nDhxQvVIk55sM2bMsNUkyQjlc+bMwdmzZ1WQJTVM2SE1R1I31hzt4WPST3CSHT6lS8JozCdOw4gi1juO3G8UF+++vb1R6fZL19Kw3vIlLl++7LYZKRL+/ZyoMPRVePvnLrNtTknGgekvufV83YHBkcEwODIWBkfGwuBI/25ncFRxiGuCo/0zPC840leel4iIiFxDM26zmj6qJImIiIhchJkjIiIicqYZN3PE4IiIiIicmDgIJBEREZEdA2eOWHNEREREZIeZIyIiInJiYrMaERERkR02qxERERGRYOaIiIiInGnGzRwxOCIiIiInpn+X3B7DE7FZjYiIiMgOM0dERETkTGOzGhEREZGNkbvys1mNiIiIyA4zR0RERORMM26zGjNHREREdPMASbvFJRdee+01mEwmDB482LYuOTkZ/fv3R2RkJEJCQvDII4/g7NmzcDUGR0RERJRlzZEpl8ut+O233/Duu++ievXqDuuHDBmClStX4rPPPsOGDRtw6tQpdOzYEa7G4IiIiIjuGFevXkXnzp3x/vvvI3/+/Lb1ly9fxocffojp06fj3nvvRZ06dTB//nxs2bIFv/zyi0vPgcERERERub5JTbvRtJaQkOCwpKSkZPm00mz2wAMPoFWrVg7rt2/fjrS0NIf1lSpVQokSJbB161aXXjqDIyIiInJrs1p0dDTCw8Nty5QpUzJ9zk8//RQ7duzIdPuZM2fg5+eHfPnyOawvXLiw2uZK7K1GREREbhUbG4uwsDDbfX9//0z3GTRoENauXYuAgADkJWaOiIiIyK3NamFhYQ5LZsGRNJudO3cOtWvXho+Pj1qk6HrWrFnqtmSIUlNTER8f7/A46a0WFRXl0ktn5oiIiIjyfITsli1bYs+ePQ7runfvruqKRo4cqZrmfH19sW7dOtWFX+zfvx/Hjx9Ho0aN4EoMjsgwtDjHbxtGoKWlwogu3m2GER2YWxtGU+HZ32Aomn7f26GhoahatarDuuDgYDWmkXV9z549MXToUERERKgM1PPPP68Co4YNG7r0XBgcERERkUeMkD1jxgx4eXmpzJH0eIuJicGcOXNc+yQMjoiIiOhODY7Wr1/vcF8Ktd955x21uBMLsomIiIjsMHNEREREeV6QfSdhcERERER3ZLNaXmFwRERERE5MmqaW3B7DE7HmiIiIiMgOM0dERETkTGOzGhEREZGNkQuy2axGREREZIeZIyIiInKmsVmNiIiIyIbNakRERESkMHNEREREzjQ2qxERERHZsFmNiIiIiBRmjoiIiMiZxmY1IiIiIl00i+UWgyMiIiJypmnXl9wewwOx5oiIiIjIDjNHRERE5MRk4N5qDI6IiIjImWbcgmw2qxERERHZYeaIiIiInJgs15fcHsMTMTgiIiIiZ5pxm9Xu2OBowYIFGDx4MOLj4+FJTCYTvvrqK3To0CGvT+WO0q7bBTza7xwiCqbj8N5AzBldDPt3BUGvOvc/is79jzusiz0ciGcfrAe9M9prXbXBFTzW9yzKV0tCZFQaxvcsg62r80FvfOJSUeDLWAT/dRmmVAvSCgbgTNfSSCkVrLZHrjyJ0N8uqf00HxOSSwTjYodiSC4dAr0x2nvciPK05qhbt24qmJDFz88P5cqVw8SJE5Genu6yACtfPvf8kRo/fjxq1qzplmPrTbOH4tBn3Cksnh6F/jEVcHhvACYvOYzwyDTo2dGDQeh8T0PbMvxp/b9fjPhaBwRZcHhvEGaPjoZeeSWmI3rq39C8vXDy+Qo4Or4azj8WDUuwt22f1MIBONepBI6NvQuxwysjPdIPxWYegPcVfb32RnqPmzTXLJ4ozzNHrVu3xvz585GSkoJVq1ahf//+8PX1RZEiRfL61O4YqampKnj0VB37XMD3SyKwZmmEuj9rZHHUb5mAmE6XsGx2YeiV2WxC3AXPfd1uhRFf699/CleLnkWsPo20/H442620bV16AX+Hfa7Uj3S4f/6xEgj/+QL8TiQhqbIv9MJQ73GNg0DmGX9/f0RFRaFkyZLo168fWrVqhRUrVti2r169GpUrV0ZISIgKpE6fPq3Wb9y4UQVRZ86ccTieNMXdfffdWL9+Pbp3747Lly/bslOS7RFxcXHo0qUL8ufPj6CgILRp0wYHDx50yjgtX74c5cuXR0BAAGJiYhAbG2vbPmHCBOzevdt2bFlndeHCBTz88MPq2PJ4++sRGzZsQP369dW1SxD44osvOmTLmjdvjgEDBqhrKVCggHpuMX36dFSrVg3BwcGIjo7Gc889h6tXr+JO5uNrQfnq17BjU6htnaaZsHNTKKrUuQY9K1YiCYvW/4IPV/+K4W/8jYJFkqFnRn6t9S74j3iklAxGkXcPocywnSgx6S+Ebzqf9QPSLQjfdA7mQG+kRAdCL/geN448D44yCgwMVJkSce3aNbz55ptYtGiRCoaOHz+OYcOGqW333HMPypQpo7ZZpaWlYfHixejRowcaN26MmTNnIiwsTAVUslgfK815v//+uwpatm7dCk3T0LZtW/V4K3nuyZMnY+HChfj5559V7dOTTz6ptj3xxBN44YUXcNddd9mOLeusJHB6/PHH8ccff6jjdu7cGZcuXVLbTp48qdbVq1dPBVdz587Fhx9+iEmTJjn8Hj7++GOVLZLnnjdvnlrn5eWFWbNm4a+//lLbf/zxR4wYMeKmv0/JyCUkJDgst1NYhBnePkD8ecckZdwFH+Qv6Jrm0zvR/j/CMP3lihjTpyremVgOhYulYOqi3QgM0u81G/W1NgLf8ykI33AOqYUCcHJgBVy+pyAKLj2GsK0XnIKocgO3o/yA7ci/7ixODK4AS4h+skZGe48buVntjgmOJED54YcfVKbo3nvvVeskWJHAoG7duqhdu7bKpqxbt872mJ49e6omOauVK1ciOTlZBSYSWISHh6usjmSmZJHsk2SIJCj64IMPVIapRo0aKqCSoEUyRVby3LNnz0ajRo1Qp04dFYxs2bIFv/76qwrg5Fg+Pj62Y8s6Kwm+OnXqpGqoXn31VZXdkceJOXPmqKyPHLtSpUqqcFuCqWnTpsFiudHnUTJOb7zxBipWrKgWIZmkFi1aoFSpUup3JAHVsmXLbvp7nTJlivo9WBd5bnK/3zdFYPPqgjh6IAQ7fo7AuL5VERyajrtb3+TbNtEdSj7gUkoE4eLDxZFSIhiX7ymEy00LqoDJ3rWKoTg2+i7EjqiMxLvCUfS9f+CdoL9aHMP1VtNyuXigPA+OvvnmGxVoSNOVNG9JBsba/CXNUmXLlrXtK01Q586dcwhCDh06hF9++UXdl6YtCYyk2Skrf//9twpqGjRoYFsXGRmpAhDZZiX7SHbHSgIZaWqz3ycr1atXt92Wc5HslfW85fEScEnQZtWkSRMVQJ04ccK2TgKyjCR4bNmyJYoVK4bQ0FA888wzuHjxospyZWXUqFGqadG6WJsGb5eES94wpwP5Mnyryl8gHXEZvn3pWeIVH5w8GoiiJfXbtMbXWr/Sw32RWsSxeUzu+8Zdz/Jbaf7eSCsUgOQyITjbpTQ0bxPCftbPFwKjvcdNzBzlHcmE7Nq1S2V0kpKSVIbGGtxITZE9CSgkw2RVqFAhtGvXTmWPzp49i++++041qeW1zM7bPiuUHRkDvKNHj+LBBx9UgdcXX3yB7du345133lHbrM2QmZG6JgnO7JfbKT3NCwf/CEKtplds60wmDTWbXsXe7cbp+hoQZEaREsm4dF6/Bdp8rfUrqWwIfM86BvZ+Z5ORFvEf72cL4JXuoZ+OmeB73DjyPNSVIECan25Vr169VBNW8eLFVZZJsjBW0rRmNpsd9pfibil+3rZtm6pLEpJ92b9/P6pUqWLbT/aRuiQpnBayXeqO5PFZHTs75PES3EiQZ80eSV2RZILkGrIiwZAEWNL8JrVH4r+a1O4UX75XAMNmxuLA7iDs3xmEh3ufV92f13x6vbeHHvUcfhjbforAuVMBiCyUgqcHHIPFbML6bwtCz4z4WkvgW7RUiu1+VHQKylS5hivxPjh/Sh/BcFyrwijx+j5ErDqFK3UjEHA0URVkn326lNpuSjEjYtVpJNbIp7JM3lfTkW/9OfjEp+JKHX299oZ6j2vG7a2W58FRbklPLsmGSP2NjJFkT2pzpLlK6pSktsjae6x9+/bo3bs33n33XRWUSG8xaaqS9fbZn+eff14VQEsTm9Q7NWzY0BYsybGPHDmisl4S1MhxJEvzX6SHmRSKy7HlmBJ0jRs3DkOHDrUFPZmRAFLqoN5++22VLbMv1L7TbViRH+GRZnQZfkYVLR7+KxAvdy6N+Av6KdTMqEDhFIx8cx/C8qXh8iVf/LUjHEM61URCnD4+LLNixNe6Qo1rmPrZjd6ufcefVD/XLIvAtKHXgwdPl1IqBKf6lUOBr04g4ttTSCvgj/OPl8CVBv923/cywe9MEsJ/uQCvq+mwBPsguVQwYodXQmpR/fRWM9p73OSCZjFPbVbz+OBIAgqpPZLCZ+meb08yQ3379lV1TJIdkiBE6pmkGW7QoEGqmUqapKTnm4yxZN8cJoHUyJEj8dRTT6libSnell5lVo888gi+/PJL1SwoGSU5ppzHf5EgTJ5r+PDhKmCLiIhQheWjR4++6eNkX+nK//rrr6s6IjlnKbbOeM13qhXzC6jFKF4fdj3DaERGe63/2BqKmOK1oXeJ1fOpJTOarxdO9ysPozDae9yITJp9EY+HkuDi/PnzTuMJGW3qkuyQrvzSa6052sPHpL9vOjfjnU/fA/Vlxhx/GYbkdWPkZiM5MFf/QVpGFZ79DUaSrqVhPb5WHWzcVUOa8O/nRKPWE+HjG5CrY6WnJWPr92Pder7u4NGZI/ll79mzB0uWLHFZYERERERgs5qnkhohGT9Ims7uu+++vD4dIiIi0gGPDo5kihB3kNqh7NQPERER6ZZFu77k9hgeyKODIyIiInITzQUjXHtmbMTgiIiIiJyZXFAzdGMuCM+S5yNkExEREd1JmDkiIiIiZxpHyCYiIiKyMXJXfjarEREREdlh5oiIiIicaeytRkRERGRj0jS15EZuH59X2KxGREREZIeZIyIiInJm+XfJ7TE8EIMjIiIicmJisxoRERERCWaOiIiIyJnG3mpEREREN3CEbCIiIqIbOEI2ERERUR6aMmUK6tWrh9DQUBQqVAgdOnTA/v37HfZJTk5G//79ERkZiZCQEDzyyCM4e/asy8+FwRERERFl3aym5XLJpg0bNqjA55dffsHatWuRlpaG+++/H4mJibZ9hgwZgpUrV+Kzzz5T+586dQodO3Z0+aWzWY2IiIicmCzXl9weI7u+//57h/sLFixQGaTt27fjnnvuweXLl/Hhhx9iyZIluPfee9U+8+fPR+XKlVVA1bBhw9ydrB1mjoiIiMitEhISHJaUlJT/fIwEQyIiIkL9lCBJskmtWrWy7VOpUiWUKFECW7duden5MjgiIiIitzarRUdHIzw83LZIfdHNWCwWDB48GE2aNEHVqlXVujNnzsDPzw/58uVz2Ldw4cJqmyuxWc2ovLwBkzeMREv+728qpA/ekde/aRpNhWd/g9EEbywII0lLTAVae944R7GxsQgLC7Ot9vf3v+nDpPbozz//xObNm5EXGBwRERGRW4WFhTkERzczYMAAfPPNN9i4cSOKFy9uWx8VFYXU1FTEx8c7ZI+kt5pscyU2qxEREVGWc6uZcrlkl6ZpKjD66quv8OOPP6J06dIO2+vUqQNfX1+sW7fOtk66+h8/fhyNGjVy6bUzc0RERER5PkJ2//79VU+0r7/+Wo11ZK0jkhqlwMBA9bNnz54YOnSoKtKWTNTzzz+vAiNX9lQTDI6IiIgoz82dO1f9bN68ucN66a7frVs3dXvGjBnw8vJSgz9Kj7eYmBjMmTPH5efC4IiIiIicadJtzAXHyO6u2cgyBQQE4J133lGLOzE4IiIiIiemHNYMZXUMT8TgiIiIiLLoyq/l/hgeiL3ViIiIiOwwc0RERER53lvtTsLgiIiIiJxZpGjIBcfwQGxWIyIiIrLDzBERERE5MbG3GhEREZEdA9ccsVmNiIiIyA4zR0RERORMM27miMEREREROdOMGxyxWY2IiIjIDjNHRERE5Mxi3HGOGBwRERGRExO78hMRERHZYc0REREREQlmjoiIiMiZRZN2sdwfwwMxOCIiIiJnGpvViIiIiIiZIyIiIsqc5oLMj2dmjhgckdtVbXAFj/U9i/LVkhAZlYbxPctg6+p80LMHOp9VS+FiKer+sYNBWPJ2Mfy+Qd/XLdp1u4BH+51DRMF0HN4biDmji2H/riDo1fxVm1G4WLLT+m8+LY45UypBz/T+WmtmDWnzryF9TTK0SxaYCnjBp00AfLsEwWRyHgAo5c0rSF+RDL8BwfB9XAe/B43NauRG48ePR82aNWFUAUEWHN4bhNmjo2EUF077Yf4bJfB8+2oY2KEqdm8Nw9h3D6BE+WvQs2YPxaHPuFNYPD0K/WMq4PDeAExechjhkWnQq0Gd66PzvXfblpf61FLrN60tBD0zwmudtuQa0r5Ogt+QEAQuioBf3xCkLUlC+hdJTvumb0yBZW+aCqDI8/FVzKV27dqhdevWmW7btGmT+nbRsWNHrFu3Dkb1+0/h+HhqUWz5Xv9ZE6ttP+bHb+vz4dTRAJw8EoiPp0Uj+ZoXKtW6Cj3r2OcCvl8SgTVLI3D8YABmjSyOlCQTYjpdgl4lxPkh7qK/bal/zwWcOh6IPb/nh54Z4bW2/JkOnyb+8GnkD68i3vBp7g/ver4w/53uuN95M1Lfugr/MWH6ao+xaK5ZPBCDo1zq2bMn1q5dixMnTjhtmz9/PurWrYvq1asjMjIyy2Okpqa6+SwpL3l5aWj24EUEBFqwb0cI9MrH14Ly1a9hx6ZQ2zpNM2HnplBUqaPvjJmVj48FLR44gzXLi7pg3oU7l1Fea6+qPjDvSIUl9nowZD6UDvOeNPg08LPto1k0pEy6At8nA+FVWk+REeTiXLN4IAZHufTggw+iYMGCWLBggcP6q1ev4rPPPlPBU8ZmtW7duqFDhw6YPHkyihYtiooVK6r1kmVavny5w3Hy5ctnO7YEUQMGDECRIkUQEBCAkiVLYsqUKbflOinnSlW8hi/3/IYV+37FgElH8Eq/Cjh+SAd1CFkIizDD2weIP+/4ARF3wQf5Czp+09arRveeR0hoOn5YIcGRfhnltfbtHASfe/2R9HQcElucR3LPOPg+FgSf+wNs+0gzG7wBn0cD8/RcybV0Fubefj4+PujSpYsKYF5++WVbkZ4ERmazGZ06dcKMGTOcHifNbGFhYSrrlF2zZs3CihUrsGzZMpQoUQKxsbFquZmUlBS1WCUkJOTo+ujWnTgcgP4PVkNwqBlN21zEC1P/wYhOlXUdIBnd/Q+fxO8/R+LSef+8PhVyAfNPKUhfmwL/saHwKuWjMkepb1+FKdILvm0CYN6fhvTPryHgg/yZFmh7PM24BdkMjlygR48emDp1KjZs2IDmzZvbmtQeeeQRhIeHZ/qY4OBgfPDBB/Dzu5Ge/S/Hjx9H+fLl0bRpU/UPUTJH/0UySxMmTMjB1ZCrpKd54fSx698wD/0ZjArVE9G+21m8Pbo09CjhkjfM6UC+DJmD/AXSEZchw6BHhYokoWaDS5g8tDr0ziivdeqcxOvZo5bX/x17lfWBdsaMtMXXVHBk2Z0GLU5D0mN2dVbm649L+zwJQcuyLqfwCBYJbIw5Qjab1VygUqVKaNy4MT766CN1/9ChQ6oYW5rUslKtWrUcBUbW5rhdu3apZriBAwdizZo1//mYUaNG4fLly7blvzJN5D7yxdLXzzPb37MbDB78Iwi1ml6xrTOZNNRsehV7t+s/W3Zf+1O4fMkPv24qAL0zymutpWjOn5LeJuDff8Y+MQEInJ8fgR/eWKS3mtQfBbyZ+Rdjj8wcablcPBCDIxeRQOiLL77AlStXVNaobNmyaNasWZb7S+YoI8kGaRneSGlpN7rF1q5dG0eOHMErr7yCpKQkPP7443j00Udvel7+/v6q+c5+ud0CgswoU+WaWkRUdIq6XbCofgvRuw0/jqr1ElCoWIqqPZL71Rsm4KcV+v7g/PK9Amjz1CW0euwSossl4/nXTqihHNZ8GgE9k8Dgvvan8cPKIrCYjfFn1QivtU9jP6Qtuob0rSmwnDar7vppS6/B5+7rX2xN4V7wKuPjsEh7jCnCC14l9JNBMyK+ei4igcqgQYOwZMkSLFy4EP369ctxG7QUdp8+fdp2/+DBg7h2zbHnhwQ3TzzxhFokMJJhBC5duoSIiDv3D1KFGtcw9bODtvt9x59UP9csi8C0oaWgR/ki0zFs2j+IKJiGxCveOLI/CKO7VcLOzTr4NnkTG1bkR3ikGV2Gn1GFuYf/CsTLnUsj/oIv9Kxmw0soVDQZa1UvNWMwwmvtNzgEqR9cQ+r0q9Dirg8C6ftQIHy76Sc7dlOaC2qGPDNxxODIVUJCQlTAIs1YUvQsTWA5de+992L27Nlo1KiRKuYeOXIkfH1v/KGZPn266qlWq1YteHl5qaLvqKgo1aPtTvbH1lDEFK8NI5n5YhkY1Yr5BdRiJDu3RqJtjVYwGr2/1qYgL/gPDAFkySaPrzOypxm3INsY+d/b2LQWFxeHmJgY1UU/p6ZNm4bo6GjcfffdeOqppzBs2DAEBd34hhIaGoo33nhDjZ1Ur149HD16FKtWrVKBEhEREbmGSctY5EK6Jlkt6UHX3KsjfEz6SX9nh5efsa5XWJKd5/wyAu+CBWFE5vPnYTTBG431WqclpmJV6w9VBxt31ZAm/Ps50apQL/h45azjUEbpllT8cO4Dt56vO7BZjYiIiJxpbFYjIiIiImaOiIiIKFOacTNHDI6IiIjImYUjZBMRERERM0dERESUGU2zqCW3x/BEDI6IiIgo83ohC2uOiIiIiOwCG2MGR6w5IiIiIrLDzBERERE5s1gAUy5rhlhzRERERLqhsVmNiIiIiJg5IiIiosxoFgu0XDarsSs/ERER6YfGZjUiIiIiYuaIiIiIMmXRAJMxM0cMjoiIiCiLwMbigmN4HjarEREREdlh5oiIiIicaBYNWi6b1TRmjoiIiEg3NItrlhx65513UKpUKQQEBKBBgwb49ddfcbsxOCIiIqLMM0eW3C85sXTpUgwdOhTjxo3Djh07UKNGDcTExODcuXO4nRgcERER0R1h+vTp6N27N7p3744qVapg3rx5CAoKwkcffXRbz4M1RwZjbf9N19JgNF4e2vadGxYDvs5Cs6TCiMwGfL3TElMNeb23o5YnXUvJ9cSx6bj+nkxISHBY7+/vrxZ7qamp2L59O0aNGmVb5+XlhVatWmHr1q24nRgcGcyVK1fUz83aylwPfOpxkvP6BOi2uZDXJ0C3TWsY9m95eHi4W47t5+eHqKgobD6zyiXHCwkJQXR0tMM6aTYbP368w7oLFy7AbDajcOHCDuvl/r59+3A7MTgymKJFiyI2NhahoaEwmUy37XnlW4P845DnDgsLg1Hwuo1z3Ua8ZqNed15es2SMJDCSv+XuEhAQgCNHjqhMjqvOOePnTcas0Z2GwZHBSIqyePHiefb88ofEKH9A7fG6jcOI12zU686ra3ZXxihjgBQQEIDbqUCBAvD29sbZs2cd1st9yWTdTizIJiIiojzn5+eHOnXqYN26dbZ1FotF3W/UqNFtPRdmjoiIiOiOMHToUHTt2hV169ZF/fr1MXPmTCQmJqrea7cTgyO6LaR9WQrw7vR2ZlfjdRvnuo14zUa9biNe8+3yxBNP4Pz58xg7dizOnDmDmjVr4vvvv3cq0nY3k+apY3sTERERuQFrjoiIiIjsMDgiIiIissPgiIiIiMgOgyMiIiIiOwyOiIiIiOwwOCIiIroF8fHxeX0K5CYMjoiIiP7D66+/jqVLl9ruP/7444iMjESxYsWwe/fuPD03cj0GR0Qu8Mcff6hh7q23b7bo1U8//QQj6tGjh5oINCMZ1Ve26V1ycrKaiNV+0aN58+bZZpZfu3atWr777ju0adMGw4cPz+vTIxfjIJDkdp9//jmWLVuG48ePO83yvGPHDuhlQl8ZzbVQoULqtsxAbf9Py3pffprNZuiRjBYskxrLMP8y/L/1g0TvZKLM06dPq9fe3oULF9Rkmenp6dCba9euYcSIEerf9cWLF5226/E9HhgYiAMHDqj39aBBg1RQ+O6776p1DRo0QFxcXF6fIrkQM0fkVrNmzVIfljL0+86dO9VcOZKKPnz4sPrGpRdHjhxBwYIFbbfl+uSndbHel596dfLkSQwYMEAFw2XKlEFMTIz68MwYEOuFZEguX76sgl7JHNlnTuSDctWqVU4Bk15IpuTHH3/E3LlzVVD8wQcfYMKECShatCgWLlwIPcqfPz9iY2PVbZnOolWrVuq2vP56DAYNTzJHRO5SsWJFbcmSJep2SEiI9s8//6jbY8aM0fr375/HZ0fusn37dm3AgAFaZGSkWp5//nlt165dmp6YTCbNy8sry8Xb21ubNGmSpkfR0dHaTz/9pG6HhoZqBw8eVLcXLlyotWnTRtMj+XtVsmRJrVWrVuo9feXKFbX+k08+0WrVqpXXp0cuxmY1cqugoCD8/fffKFmypPoWLe30NWrUwMGDB9GwYcNMU/KebsWKFZmulya1gIAAlCtXDqVLl4benTp1Cu+99x5ee+01+Pj4qGaIRo0aqdqNu+66C55uw4YNKmtw77334osvvkBERIRtm5+fn3rPSyZFj0JCQrB3716UKFFCNaV++eWXKiss2dFq1arh6tWr0Ju0tDS89dZbKnvUrVs31KpVS62fMWMGQkND0atXr7w+RXIhH1cejCgjqbm4dOmS+qCQP6S//PKLCo7kj6he4/IOHTo41RxlrDtq2rQpli9frlL1evsA+frrr/HRRx+pQLhu3bqYPXs2OnXqpGbaHj16NB577DH1werpmjVrpn7Ke1ne2/K6GoU0m1qvu1KlSqr5VIKjlStXIl++fNAjX19fDBs2zGn9kCFD8uR8yL2YOSK3km9TUsA4btw4vPPOO6pWoUmTJvj999/RsWNHfPjhh9CbdevW4eWXX8bkyZPVB4b49ddfMWbMGBUchIeH49lnn1VFnHq6/ueffx6ffPKJCgCfeeYZ9dpXrVrVYR8pWpdsirVnnx5I/YlkUiTgFfI+f//991GlShV1W28BsDVbIoXoAwcOxA8//IB27dqp112C4+nTp6uCZb35r1qqLl263LZzIfdjcERuJR+Cskizivj000+xZcsWlC9fXgUI0vygNxIQSHNS48aNHdb//PPP6NOnD/766y/1gSLdvKUHn160bNlSBUQS9EqRbmak55b8HqxZFz2QZiQZA6dt27bYs2ePypa98MILamgDyarMnz8fenfs2DFs375dNRlXr14depQxyJVAUHrtyd8wKR+QDDnpB4MjIjd0+f3tt9+csibywSmZpKSkJPVhUrlyZfXHlTybZI3+/PNPlCpVCuPHj1e3pceeDFMhAZNky/REgoLWrVur2jH5kmNkUjvZr18/lRGX3pmkH6w5IpeTgQ4lMJDxfv5r0EM9fsusU6eO+mMpaXhr936pt5FxYerVq2f7o6rHcYDkuiRjcu7cOaems7Fjx0KPJHNgDXIlI2htXpECbT0OiCi1N3oezDQnJDiUDgdPP/009u3bl9enQy7EzBHd9gERrfQ6IOL+/fvRvn17VbBqDYCkh4sUsUqxcoUKFVQxtoyNI7U5eiF1NvItukCBAqoQ375AWW7rZcDPjB566CE1lpPU0r3yyivqdZcpJdasWaPGfZJBAvVGipCl6VQCA6PbtWsX7rnnHl0GwkbG4IhcTpqMrL135PbNSC82PZKsiXw4Wj8YK1asiPvuu08Fi3olr+Vzzz2HkSNHwkikbkyuWwJgKVDu2bOnLYCQ4F8GQtUbKb6XzKhkTiRTGhwc7LBdirL1PkSHfHTKyOjSG1O+BMlUIqQfDI7IrbUJUnQtvbSMMK6P0YWFhalv0ZIhI31r0aJFltvkS5GMnq03Gb/YyHVKs7mMczVt2jQUKVIkz86NXI/BEbmVdFuXD0yjBUfSnV+WzGpvZAwgPZKMidRU9e3bF0bzzz//qF5p8lMGCpQmZckkSAZVDwNeEhkNC7LJ7QMiSn2NkQZKkzmmJk6cqLp0y7dJPQ8OaN9kJN24JUsoA31K93Yp3LUnTU56JCNlyzyBUnO0ceNGNb6VBEe7d+9W41hJzzU9O3HihPopI2XrOQsuwzJ88803qpcp6R8zR+RWkyZNUilnGQMns9oEPX5gSkD0xhtv6KrYOivZzQhKgKjXSXdlShQZ9Xvo0KFqGgkJiqRpUQb+lDGfrMGDnkg21Ppv2zpViFy7jO8kA6DqsbZOiuylNyKDI2NgcER59uGp1w/MyMhI9cFYtmzZvD4Vuk3jHMkYVvJetw+Ojh49qrINMqec3owaNUplxSRLKhkzsXnzZjXOU+/evVX2TG9effVV1cHigw8+sA1qS/rFV5jcSro1G42MEr1kyRLVxGQk0pQoc0/JaMH2ZNDLqVOn6nacI5lLTHotZfwisHPnTpVt0KOPP/5YBQkyjIH9mGVyvdJzT4/BkQzsKnWE0gtVmo0zZsFl8l3SD2aO6LaQcWAkUJJsit6/dcm8UtLNWT4sZMlYe6PHbs5C5tqSIEHqbexdvHhRrdPjmFZCAsJt27bhs88+U2NYyXhOZ8+eVYNByiLzCupNQECAGghSrjfjGF81a9ZUAbHedO/e/abbjTBNjJEwOCK3kpGDZUwU+aYpJC0tTQ6yTr5lvvjii9AbI3ZzFlJnIkGBdVRwK7neJ554Qo0SrtfAXwZ7XLBggZo7ToJ/CQSfeuoptU6CRr2RSZNlyTiGk/y7lgyLFOUTeTJ9f4WnO6I2QWow1q9fr+ZjsmrVqpWqT9BjcCTTZxiJTMgpQZ8skkmw750nQYIU7Oqxe78UJUtzoQwOKAGSFOA/8sgj6npr1aql63nHpMPBAw88oAqUpSBdbN26VQ2EuWrVqrw+PaJcY+aI3D5q8tKlS9GwYUOHYtVDhw6hdu3aHHJfByQrKH9GevTogZkzZ6qxreznHZMJWa0foHoiU4VIgC+Bvkw2vHr1anTq1Em341hldOrUKbzzzju2OcWkF5fUGxUtWhR6IX+jpM5IvgBIwHuzYTn0Oj2OUTFzRG4lTSkZa1BEYmKibsf/kWa1m12b3prVunbtqn5KQXLjxo2daqz0SurK5syZo0aBF5JFkWyKFCrrsSt7RhIE6bHw2p7MkShBoARHMmYbGQeDI3IrGQjx22+/VbUIwho0yAeIHrMJQgpSMw4gJ6OE//nnn7ZAQi/sM3/yzVoKcbMqxpXpRfQ2p1rbtm1t9yWDJO9v+TDV84CIVvHx8WrIisxGgZdCdD2QYnoJdGXkdxkBXjKDkgEn/WNwRG4fG0RGD967d68qVpWpFeT2li1b1MjCejRjxoxM10sTjHXAPD11Y/+vDKA0uck+euutJu9n6bVlT7JmEgzr3cqVK9G5c2f1fpag1/49ILf1EhwJ+TslPdGkV6IM9Pnoo4+qQOnuu+/O61MjN2LNEbmdzDf12muvqXoj+WMq7fgyc7uMFWIkUmdVv359XLp0CXqRkwC3WbNm0BPJKEjg7+/v7xA0yESk9mPg6HH8Gym8l6yZfPnJOK6VXkkpwLJly1QPxE2bNqnpciRIkmxwVFRUXp8euRiDI6LbZNGiRSoolGYX0v+4N3oe/0aCPxkVXDpXGJF80ZHXVf5NnzlzRvXElV6LpB8MjsitjDgwoMynZU/+icnv4Pfff1ejZutxUMCMY1tJPY50b7cnA2KSft7jTz75JB5//HEYlWSSFi9erIYrkforPf4tMzLWHJFbZRV7p6SkqG7eeiLzxEm3dfuu7Nbml4oVK6rpNe6//37ouWeiZFO+++67TLfzw8Oz2WdGpFfe8OHDVf2gNI9n7KFoP62I3mzcuFEN1/DFF1+of9sSIErzGukLM0fkFtaRc4cMGaLGg5HJOe0/JOUPjEzMKfNP6TVLJqNCy++hcOHCMAIp0D127Jga66h58+b46quv1IjZ1tnb5QOVPFd2hyfQY/G9NIVLrZEs0qQmQ1ZIQCSBUcY51kgfmDkit/bYkth73rx5DlMoWAcGlPV6kvF7hmRQJPVuFDJ+09dff62Gb5APUhkA9L777lO9maZMmcLgyMNl7K5vFFJ0L2NYFShQQPXCk8FOJRNM+sbgiNxCJpm1DogovXVkEDWjMVpSVgJBa9ZMXm9pZpNeTdLswtGD9TcApmRG7XvqCakz+/TTT3XVlV+aDD///HM8+OCDupwnjzKn/2FcKc/nGTNKYGSdXyzjOqOQb9MyK7uoUaMG3n33XZw8eVJlCIsUKZLXp0cuJLVlly9fdlp/5cqVbPfi86RaKxkpm4GRsTBzRG4lE3HK2D7ShT3jxJUye/dnn30GPWWKunXrZvs2nZycrCZczViToMdxb8SgQYNUzZWQHnnSvVl680gzqtRqkH5YB/bM6MSJE04dEog8EQuyya0KFiyoalEyDvgoY6TIdAtSsKsXRh73Jqsu/TIpaYkSJVS9Bnk+6+SrMqDrXXfdBR+fG9+vpQhbmtMlKJbBEok8GTNH5FYyInZmXfalHd9+Xi49MErQ81+k7kQ+JMuWLatGQyf9sE6+KnMFxsTEOPRCtXa0kGwxkadjcERuJRmjpUuXYuzYsQ7rpWizSpUqeXZe5J5MkUww/PHHH6v7Bw4cUCMoy7pixYrhxRdfzOtTpFyyDmAqQZAUZGecW45ILxgckVvJiNAymq7MryZzTol169bhk08+0VW9EUGNFCzNLevXr1dNK1bSfCqT7jI40g+ZT0zIqO9///23ui1fdurUqZPHZ0bkGgyOyK3atWuH5cuXqwkqpTtsYGCgmkZCxg3R20SkRievs2QJGzZs6FCsK7UpEhyTfkgvRJk+5Oeff0a+fPnUOplCQwZHlKxw8eLF8/oUiXKFXfnJ7WTwP/kjKuPgXLhwQRVoMzDSHxnXKOMcekJedyMNaWAEMjp0WlqayhpdunRJLXJbBors1atXXp8eUa4xOCIil5CRsb/99lvbfWtA9MEHH6BRo0Z5eGbkahs2bMDcuXMdRoqW22+//baaGojI07FZjdxKuvfKVCLStTezmdrlGyfpgzSdylQLMhlpeno63nrrLXV7y5Yt6sOU9CM6OlpljjL79160aNE8OSciV2LmiNxqwoQJmD59uurZIiPqDh06VBVoy9xbUqRL+tG0aVPVxVsCI+mluGbNGtXMtnXrVhbq6szUqVNVL0QpyLaS2zIQ6Jtvvpmn50bkChwEktxKxrqRmeml7ig0NFR9eFrX/fLLL1iyZElenyLlUnbHq5IJaEkfZEogGbpBAmHrQJDW2xlHhGd2mDwRm9XIrc6cOWMbHVsGjLPOxySTOEo3f/J80lvpZgXX1qkmpMmF9GHmzJl5fQpEbsXgiNxKuvTKfFsyhYRkjKSpRUZNlnnVMs7oTZ47ubB9INS2bVtVhC0DP5K+xzki0isGR+RWDz/8sBr0sUGDBqpG4emnn8aHH36oirOHDBmS16dHLpBxWAaZvVzGOpLRsUm/JBMoY1tZB4GU8aweeughzl5PusCaI7qtpM5Iei+VL19eDRBJ+iO1ZTJSNoMj/Tp06JDKEMpgkNbu/Pv371e92GQ4B8kSE3ky9lYjt7p48aLtdmxsLFatWqWa2cLDw/P0vIjo1g0cOFAFQPJveseOHWqRbHDp0qXVNiJPx8wRucWePXtUZkj+eEqWSKYUkPm2ZLRk6cYvP2U6Eess36SvzNEff/yhPihJn6RHmmSBrZ0trCRj2KRJE1y9ejXPzo3IFVhzRG4xYsQI9Ydz8eLFWLRokeqdJt3533//fbVd6o9ee+01Bkc6IONW2UtOTkbfvn2dunR/+eWXt/nMyF2kM8WVK1ec1ktQ5OfnlyfnRORKzByRWxQoUEDNoSaTzMofTBnjRnqoWQcD3LdvnyralckqybN17949W/vNnz/f7edCt0eXLl1UU5p0rqhfv75at23bNvTu3Vv9G1+wYEFenyJRrjA4IreQpjMZ48g6EWnGIt2zZ8+qaQY49g2R55EvNdKdf+XKlfD19bUNAim91SQIlrGviDwZm9XIbTIODMiZ2Yn0QYKfr7/+WvVas3blr1y5MsqVK5fXp0bkEgyOyG26detmG+gxYx1KSkpKHp8dEd2qiRMnYtiwYSoYsg+IkpKS1LxrY8eOzdPzI8otNquRW7AOhUi/ZKBHGZLD2mxuP3SHrGNzOXk6Zo7ILRj0EOmXdb68jKSuMCIiIk/OiciVGBwREVG25M+fXwVFslSoUMEhQJJskfRMleZzIk/HZjUiIsqWjz/+WGWNevTogZkzZzqMdC/jG5UqVQqNGjXK03MkcgUGR0RElCMbNmxQI2H7+LDxgfSJc6sREVGOSG+0JUuWqN5pRHrE4IiIiHKkVq1aqit/VFSUGhVb5lkj0hMGR0RElCNSb3Tq1CnVK/XcuXO45557UKVKFbz55ptq9HsiT8eaIyIiyhUJkN577z1MnjxZ9Vpr27YtBg4ciHvvvTevT43oljBzREREt+zXX3/FuHHjMG3aNDUA5KhRo9TE0w8++KBqeiPyRMwcERFRjjNFixYtUs1qBw8eRLt27dCrVy/ExMTYxj7avHkzWrdurcY+IvI07IdJREQ5Urx4cZQtW1aNdyRzKBYsWNBpn+rVq6NevXp5cn5EucXMERER5cimTZtw99135/VpELkNgyMiIiIiOyzIJiKiHJHu+s888wyKFi2qRsn29vZ2WIg8HWuOiIgoR6TO6Pjx4xgzZgyKFCniMAEtkR6wWY2IiHIkNDRU1R3VrFkzr0+FyC3YrEZERDkSHR0Nfq8mPWNwREREOZ4+5MUXX8TRo0fz+lSI3ILNakRE9J/y58/vUFuUmJiI9PR0BAUFwdfX12HfS5cu5cEZErkOC7KJiChb2SIio2DmiIiIskUmlX3zzTexYsUKpKamomXLlmpetcDAwLw+NSKXYs0RERFly6uvvoqXXnoJISEhKFasGN566y30798/r0+LyOWYOSIiomwpX748hg0bhmeffVbd/+GHH/DAAw8gKSkJXl78rk36weCIiIiyxd/fH4cOHVJd+a0CAgLUOpmMlkgvGOoTEVG2SO80CYbsSU+1tLS0PDsnIndgbzUiIsoWaWiQqUMkg2SVnJyMvn37Ijg42Lbuyy+/zKMzJHINBkdERJQtXbt2dVr39NNP58m5ELkTa46IiIiI7LDmiIiIiMgOgyMiIiIiOwyOiIiIiOwwOCIiIiKyw+CIiG476Q7eoUMH2/3mzZtj8ODBt/081q9fr2aaj4+Pz3If2b58+fJsH3P8+PGoWbNmrs7r6NGj6nl37dqVq+MQ0a1hcEREtoBFPpBl8fPzQ7ly5TBx4kQ18J+7ybg4r7zyissCGiKi3OA4R0Rk07p1a8yfPx8pKSlYtWqVmlRURkAeNWqU074yK7sEUa4QERHhkuMQEbkCM0dEZCMjH0dFRaFkyZLo168fWrVqhRUrVjg0hU2ePBlFixZFxYoV1frY2Fg8/vjjyJcvnwpy2rdvr5qFrMxmM4YOHaq2R0ZGYsSIEWqkZXsZm9UkOBs5cqSaw0vOSbJYH374oTpuixYt1D758+dXGSQ5L2GxWDBlyhSULl0agYGBqFGjBj7//HOH55GAr0KFCmq7HMf+PLNLzkuOERQUhDJlymDMmDGZTp/x7rvvqvOX/eT3c/nyZYftH3zwASpXrqym46hUqRLmzJmT43MhIvdgcEREWZIgQjJEVuvWrcP+/fuxdu1afPPNNyooiImJQWhoKDZt2oSff/4ZISEhKgNlfdy0adOwYMECfPTRR9i8eTMuXbqEr7766qbP26VLF3zyySeYNWsW/v77bxVoyHEl2Pjiiy/UPnIep0+fxltvvaXuS2C0cOFCzJs3D3/99ReGDBmiRm/esGGDLYjr2LEj2rVrp2p5evXqhRdffDHHvxO5VrmevXv3qud+//33MWPGDId9ZCLWZcuWYeXKlfj++++xc+dOPPfcc7btixcvxtixY1WgKdf36quvqiDr448/zvH5EJEbyAjZRERdu3bV2rdvr25bLBZt7dq1mr+/vzZs2DDb9sKFC2spKSm2xyxatEirWLGi2t9KtgcGBmqrV69W94sUKaK98cYbtu1paWla8eLFbc8lmjVrpg0aNEjd3r9/v6SV1PNn5qefflLb4+LibOuSk5O1oKAgbcuWLQ779uzZU+vUqZO6PWrUKK1KlSoO20eOHOl0rIxk+1dffZXl9qlTp2p16tSx3R83bpzm7e2tnThxwrbuu+++07y8vLTTp0+r+2XLltWWLFnicJxXXnlFa9Sokbp95MgR9bw7d+7M8nmJyH1Yc0RENpINkgyNZISkmeqpp55Sva+sqlWr5lBntHv3bpUlkWyKPZmM9J9//lFNSZLdadCggW2bj48P6tat69S0ZiVZHW9vbzRr1izb5y3ncO3aNdx3330O6yV7VatWLXVbMjT25yEaNWqEnFq6dKnKaMn1Xb16VRWsh4WFOexTokQJFCtWzOF55Pcp2S75Xclje/bsid69e9v2keOEh4fn+HyIyPUYHBGRjdThzJ07VwVAUlckgYw9+5nXhQQHderUUc1EGRUsWPCWm/JySs5DfPvttw5BibCfQT63tm7dis6dO2PChAmqOVGCmU8//VQ1Heb0XKU5LmOwJkEhEeU9BkdE5BD8SPFzdtWuXVtlUgoVKuSUPbEqUqQItm3bhnvuuceWIdm+fbt6bGYkOyVZFqkVkoLwjKyZKyn0tqpSpYoKgo4fP55lxkmKn63F5Va//PILcmLLli2qWP3ll1+2rTt27JjTfnIep06dUgGm9Xm8vLxUEXvhwoXV+sOHD6tAi4juPCzIJqJbJh/uBQoUUD3UpCD7yJEjahyigQMH4sSJE2qfQYMG4bXXXlMDKe7bt08VJt9sjKJSpUqha9eu6NGjh3qM9ZhS4CwkOJFeatIEeP78eZWJkaaqYcOGqSJsKWqWZqsdO3bg7bffthU59+3bFwcPHsTw4cNV89aSJUtUYXVOlC9fXgU+ki2S55DmtcyKy6UHmlyDNDvK70V+H9JjTXoCCsk8SQG5PP7AgQPYs2ePGkJh+vTpOTofInIPBkdEdMukm/rGjRtVjY30BJPsjNTSSM2RNZP0wgsv4JlnnlHBgtTeSCDz8MMP3/S40rT36KOPqkBKurlLbU5iYqLaJs1mElxITzPJwgwYMECtl0EkpceXBB1yHtJjTprZpGu/kHOUnm4ScEk3f+nVJr3EcuKhhx5SAZg8p4yCLZkkec6MJPsmv4+2bdvi/vvvR/Xq1R266ktPOenKLwGRZMok2yWBmvVciShvmaQqO4/PgYiIiOiOwcwRERERkR0GR0RERER2GBwRERER2WFwRERERGSHwRERERGRHQZHRERERHYYHBERERHZYXBEREREZIfBEREREZEdBkdEREREdhgcEREREeGG/wNF1GNsQNlWjgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "true_y = prepared_test[:][\"label\"]\n",
    "test = prepared_test[:][\"pixel_values\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test = test.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    logits = model(test).logits\n",
    "    pred_y = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "\n",
    "# Get the class names from your ImageFolder structure\n",
    "class_names = labels.names\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_y, pred_y)\n",
    "\n",
    "# Display the confusion matrix with class names\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "cm_display.plot(xticks_rotation='vertical', values_format='d') # Rotate x-axis labels for better readability\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
