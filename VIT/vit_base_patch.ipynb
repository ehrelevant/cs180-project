{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12000005",
   "metadata": {},
   "source": [
    "This notebook uses the model from https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "\n",
    "All seeds have already been set to 42, however if you will run torch using cuda, expect there to still be slight variations in the results if you rerun it. This is due to torch using non-deterministic algorithms (these are 3x faster than deterministic ones for this model at least from my testing)\n",
    "\n",
    "Based on the code from \"may15_VIT3.ipynb\" in old\n",
    "\n",
    "Overall:\n",
    "- uses 8 epochs, should take 30 minutes at most\n",
    "    - could be improved setting it so that the model trains forever (like say until 20 epochs) until the validation loss plateus\n",
    "- has L2 regularization\n",
    "    - ctrl+f \"weight_decay\" to find the line that sets this\n",
    "- NO dropouts \n",
    "    - from testing may15_VIT2 VS may15_VIT3, applying 0.2 dropout made the validation loss slightly bigger, but maybe different values of dropout (0.1, or 0.5) would do better\n",
    "- uses the initial image preprocessing transforms from may13_VIT1.ipynb\n",
    "    - this achieves the lowest training loss with lower validation loss as well, but it has the biggest gap (difference) between training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01cae6",
   "metadata": {},
   "source": [
    "Useful documentation links for finding what parameters we can change:\n",
    "\n",
    "HuggingFace:\n",
    "- https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.ViTConfig\n",
    "\n",
    "PyTorch:\n",
    "- https://docs.pytorch.org/vision/0.9/transforms.html\n",
    "- https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae52cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99eeeb8d",
   "metadata": {},
   "source": [
    "May 15, changes by justin\n",
    "- initial learning rate set to 1e-4\n",
    "- lr scheduler: cosine\n",
    "- early stopping callback with patience = 5, threshold = 0.001\n",
    "\n",
    "best model: epoch 5, step 680\n",
    "\n",
    "results: may13_VIT1 is still better in terms of loss and f1-score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce118d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if running in Google Colab, do these\n",
    "# !pip install datasets=='3.5.1' evaluate torch torchvision transformers Pillow numpy scikit-learn 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99de451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import os\n",
    "\"\"\"\n",
    ".venv/Scripts/activate\n",
    "\n",
    "python -m image_process\n",
    "\"\"\"\n",
    "base_output_dir = \"model\" ## if you wanna save different models, just make a new git branch, saving a VIT model takes up A LOT of space\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "dataset = load_dataset(\"potato_train/train\")\n",
    "filenames_ds = load_dataset(\"potato_train/train\").cast_column(\"image\", Image(decode=False))\n",
    "\n",
    "filename_col = [x['image']['path'].split('\\\\')[-1] for x in filenames_ds['train']]\n",
    "dataset['train'] = dataset['train'].add_column(\"filename\", filename_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadebc02",
   "metadata": {},
   "source": [
    "We retrieve the feature extractor from our desired VIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f143290-9281-4f29-92d0-f9013f809519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "# import model\n",
    "model_id = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "# feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e548a6",
   "metadata": {},
   "source": [
    "These are the steps used to preprocess the images and to perform data augmentation on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2617bbc7-5778-475f-a8fd-3be9fa1d5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    # RandomRotation,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    RandomAffine,\n",
    "    # Pad,\n",
    "    # RandomCrop\n",
    ")\n",
    "from PIL import Image  # Import PIL for RandomAffine's resample\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seeds(seed)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])\n",
    "\n",
    "training_transforms = Compose([\n",
    "    Resize(size),\n",
    "    CenterCrop(size),\n",
    "    # RandomRotation((-30, 30)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), interpolation=Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def training_image_preprocess(batch):\n",
    "    batch[\"pixel_values\"] = torch.stack([training_transforms(img) for img in batch[\"image\"]])\n",
    "    return batch\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = feature_extractor(batch['image'], return_tensors='pt')\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cfd97ad-58b8-4429-8bac-f0639a92b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94175340",
   "metadata": {},
   "source": [
    "We split the dataset. 80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c4974f0-9cdb-4fa2-90ba-67a69a98b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "dataset_train = train_test_split[\"train\"]\n",
    "dataset_test = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb414c3b-07e5-4478-a882-21dbe846e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " ClassLabel(names=['Bacteria', 'Fungi', 'Healthy', 'Pest', 'Phytopthora', 'Virus'], id=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5514339e-7fcc-4c40-aa3c-8d198423a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train = dataset_train.with_transform(training_image_preprocess)\n",
    "prepared_test = dataset_test.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb978-d838-4ba0-8259-32618f3a1b83",
   "metadata": {},
   "source": [
    "Save images of preprocessed images (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8ef810-fc8e-4ff6-af4d-138072228ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def save_unnormalized_images(prepared_dataset, raw_dataset, directory: str):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for index, item in enumerate(prepared_dataset):\n",
    "        if index >= 10:\n",
    "            break\n",
    "        pixel_values = item[\"pixel_values\"]\n",
    "        image = to_pil_image(pixel_values)\n",
    "        label_filename = raw_dataset[index][\"filename\"]\n",
    "\n",
    "        name_without_extension, extension = os.path.splitext(label_filename)\n",
    "        filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        image.save(filepath)\n",
    "\n",
    "\n",
    "save_unnormalized_images(prepared_train, dataset_train, f\"{base_output_dir}/preprocessed_train_images\")\n",
    "save_unnormalized_images(prepared_test, dataset_test, f\"{base_output_dir}/preprocessed_test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ab1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=p.label_ids,\n",
    "        )\n",
    "    )\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=p.label_ids, average=\"weighted\"))\n",
    "    return results\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7152c39",
   "metadata": {},
   "source": [
    "Training Arguments, you can apply stuff like \n",
    "- L1 or L2 regularization\n",
    "- hidden dropouts\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2653db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dropout=0.0, attention dropout=0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments, ViTConfig\n",
    "#from transformers.trainer_utils import IntervalStrategy, SchedulerType\n",
    "#from transformers.training_args import OptimizerNames\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=base_output_dir,\n",
    "  per_device_train_batch_size=16,\n",
    "  eval_strategy=\"epoch\",  # Evaluate at the end of each epoch for early stopping\n",
    "  save_strategy=\"epoch\",\n",
    "  num_train_epochs=50,  # Set a large number of epochs, early stopping will handle it\n",
    "  logging_steps=100,\n",
    "  learning_rate=1e-4, ## we could try applying learning rate cosine smthg keme\n",
    "  save_total_limit=2,\n",
    "  seed=seed,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    "  weight_decay=0.01,  # Add this line to apply L2 regularization\n",
    "  lr_scheduler_type=\"cosine\",  # Use cosine learning rate scheduler\n",
    "  warmup_steps=int(0.1 * (len(prepared_train) / 16) * 100), # 10% of the first epoch for warmup\n",
    "  # ^ Adjust warmup_steps based on your dataset size and batch size\n",
    ")\n",
    "config = ViTConfig.from_pretrained(model_id)\n",
    "config.num_labels = len(dataset_train.features['label'].names)\n",
    "# If you want to change it (do this BEFORE loading the model with from_pretrained):\n",
    "# config.hidden_dropout_prob = 0.2\n",
    "# config.attention_probs_dropout_prob = 0.2\n",
    "print(f\"hidden dropout={config.hidden_dropout_prob}, attention dropout={config.attention_probs_dropout_prob}\")\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_id,  # classification head\n",
    "    config=config,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9b483da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='6800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/6800 1:09:26 < 4:38:12, 0.33 it/s, Epoch 10/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.736800</td>\n",
       "      <td>1.513331</td>\n",
       "      <td>0.597786</td>\n",
       "      <td>0.538326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.421700</td>\n",
       "      <td>0.899073</td>\n",
       "      <td>0.789668</td>\n",
       "      <td>0.785758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.590421</td>\n",
       "      <td>0.845018</td>\n",
       "      <td>0.839725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.465100</td>\n",
       "      <td>0.480438</td>\n",
       "      <td>0.857934</td>\n",
       "      <td>0.858274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.399213</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.884025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.503302</td>\n",
       "      <td>0.843173</td>\n",
       "      <td>0.840912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.468005</td>\n",
       "      <td>0.843173</td>\n",
       "      <td>0.841520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.461604</td>\n",
       "      <td>0.857934</td>\n",
       "      <td>0.859015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.444649</td>\n",
       "      <td>0.867159</td>\n",
       "      <td>0.864899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.216100</td>\n",
       "      <td>0.464152</td>\n",
       "      <td>0.863469</td>\n",
       "      <td>0.862233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         10.0\n",
      "  total_flos               = 1562537366GF\n",
      "  train_loss               =       0.5692\n",
      "  train_runtime            =   1:09:28.89\n",
      "  train_samples_per_second =       25.966\n",
      "  train_steps_per_second   =        1.631\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "trainer.save_model() # save tokenizer with the model\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "\n",
    "trainer.save_state() # save the trainer state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a889ff",
   "metadata": {},
   "source": [
    "Retrieve the saved model from the directory, then run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e959365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Documents\\GitHub\\cs180-project\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy               =     0.8856\n",
      "  eval_f1                     =      0.884\n",
      "  eval_loss                   =     0.3992\n",
      "  eval_model_preparation_time =      0.002\n",
      "  eval_runtime                = 0:00:23.50\n",
      "  eval_samples_per_second     =     23.063\n",
      "  eval_steps_per_second       =      2.894\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(base_output_dir)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(base_output_dir)\n",
    "\n",
    "# Define the Trainer for evaluation\n",
    "# (if you don't wanna load the model from the directory and use the trained model directly, comment out the trainer line here)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
