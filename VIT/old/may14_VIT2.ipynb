{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12000005",
   "metadata": {},
   "source": [
    "Change from May 13\n",
    "- L2 regularization (training args)\n",
    "- Preprocessing images for train changed alot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99de451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Desktop\\CS_180\\cs180_project_TEMP SANDBOX\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "\"\"\"\n",
    ".venv/Scripts/activate\n",
    "\n",
    "python -m image_process\n",
    "\"\"\"\n",
    "base_output_dir = f\"models/may14_VIT2\"\n",
    "dataset = load_dataset(\"potato_train/train\")\n",
    "filenames_ds = load_dataset(\"potato_train/train\").cast_column(\"image\", Image(decode=False))\n",
    "\n",
    "filename_col = [x['image']['path'].split('\\\\')[-1] for x in filenames_ds['train']]\n",
    "dataset['train'] = dataset['train'].add_column(\"filename\", filename_col)\n",
    "\n",
    "#print(dataset['train'][0])\n",
    "#base_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f143290-9281-4f29-92d0-f9013f809519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "# import model\n",
    "model_id = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "# feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfd97ad-58b8-4429-8bac-f0639a92b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2617bbc7-5778-475f-a8fd-3be9fa1d5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomRotation,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    RandomAffine,\n",
    "    Pad,\n",
    "    RandomCrop\n",
    ")\n",
    "from PIL import Image  # Import PIL for RandomAffine's resample\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])\n",
    "\n",
    "training_transforms = Compose([\n",
    "    Resize(size),\n",
    "    # CenterCrop(size),\n",
    "    RandomRotation((-30, 30)),\n",
    "    RandomHorizontalFlip(),\n",
    "    Pad(10),  # Add padding before random crop\n",
    "    RandomCrop(size),\n",
    "    # RandomVerticalFlip(),\n",
    "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.9, 1.1), saturation=(0.9, 1.1)),\n",
    "    RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), interpolation=Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def training_image_preprocess(batch):\n",
    "    batch[\"pixel_values\"] = torch.stack([training_transforms(img) for img in batch[\"image\"]])\n",
    "    return batch\n",
    "\n",
    "def preprocess(batch):\n",
    "    # take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor(\n",
    "        batch['image'],\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4974f0-9cdb-4fa2-90ba-67a69a98b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "dataset_train = train_test_split[\"train\"]\n",
    "dataset_test = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb414c3b-07e5-4478-a882-21dbe846e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " ClassLabel(names=['Bacteria', 'Fungi', 'Healthy', 'Pest', 'Phytopthora', 'Virus'], id=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5514339e-7fcc-4c40-aa3c-8d198423a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training dataset\n",
    "prepared_train = dataset_train.with_transform(training_image_preprocess)\n",
    "# ... and the testing dataset\n",
    "prepared_test = dataset_test.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb978-d838-4ba0-8259-32618f3a1b83",
   "metadata": {},
   "source": [
    "Save images of preprocessed images (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8ef810-fc8e-4ff6-af4d-138072228ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "output_dir = f\"{base_output_dir}/preprocessed_train_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for index, item in enumerate(prepared_train):\n",
    "    if index >= 10:\n",
    "        break\n",
    "    pixel_values = item[\"pixel_values\"]\n",
    "    image = to_pil_image(pixel_values)\n",
    "    label_filename = dataset_train[index][\"filename\"]\n",
    "\n",
    "    name_without_extension, extension = os.path.splitext(label_filename)\n",
    "    filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    image.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0488fd7e-7837-4a6a-bea1-a02a00e3fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{base_output_dir}/preprocessed_test_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for index, item in enumerate(prepared_test):\n",
    "    if index >= 10:\n",
    "        break\n",
    "    pixel_values = item[\"pixel_values\"]\n",
    "    image = to_pil_image(pixel_values)\n",
    "    label_filename = dataset_test[index][\"filename\"]\n",
    "\n",
    "    name_without_extension, extension = os.path.splitext(label_filename)\n",
    "    filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    image.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ab1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=p.label_ids,\n",
    "        )\n",
    "    )\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=p.label_ids, average=\"weighted\"))\n",
    "    return results\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7152c39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2653db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments, ViTConfig\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=base_output_dir,\n",
    "  per_device_train_batch_size=16,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=8,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=5e-5,\n",
    "  save_total_limit=2,\n",
    "  seed=42,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    "  weight_decay=0.01,  # Add this line to apply L2 regularization\n",
    ")\n",
    "\n",
    "config = ViTConfig.from_pretrained(model_id)\n",
    "\n",
    "# If you want to change it (do this BEFORE loading the model with from_pretrained):\n",
    "#config.hidden_dropout_prob = 0.2\n",
    "#config.attention_probs_dropout_prob = 0.2\n",
    "config.num_labels = len(dataset_train.features['label'].names)\n",
    "\n",
    "print(config.hidden_dropout_prob)\n",
    "print(config.attention_probs_dropout_prob)\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_id,  # classification head\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b483da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1088' max='1088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1088/1088 27:32, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.772800</td>\n",
       "      <td>0.742843</td>\n",
       "      <td>0.815498</td>\n",
       "      <td>0.808637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.500782</td>\n",
       "      <td>0.867159</td>\n",
       "      <td>0.867078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.516793</td>\n",
       "      <td>0.830258</td>\n",
       "      <td>0.834309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.445508</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.860376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.399145</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>0.877705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.442838</td>\n",
       "      <td>0.852399</td>\n",
       "      <td>0.852371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.881919</td>\n",
       "      <td>0.880662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.475556</td>\n",
       "      <td>0.850554</td>\n",
       "      <td>0.852158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.407936</td>\n",
       "      <td>0.883764</td>\n",
       "      <td>0.882607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.444724</td>\n",
       "      <td>0.863469</td>\n",
       "      <td>0.862450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          8.0\n",
      "  total_flos               = 1250029893GF\n",
      "  train_loss               =       0.3178\n",
      "  train_runtime            =   0:27:34.51\n",
      "  train_samples_per_second =       10.468\n",
      "  train_steps_per_second   =        0.658\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "# save tokenizer with the model\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "\n",
    "# save the trainer state\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e959365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy               =     0.8819\n",
      "  eval_f1                     =     0.8807\n",
      "  eval_loss                   =     0.3768\n",
      "  eval_model_preparation_time =      0.001\n",
      "  eval_runtime                = 0:00:23.24\n",
      "  eval_samples_per_second     =     23.316\n",
      "  eval_steps_per_second       =      2.925\n",
      "{'eval_loss': 0.37680020928382874, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.8819188191881919, 'eval_f1': 0.8806623861457068, 'eval_runtime': 23.2454, 'eval_samples_per_second': 23.316, 'eval_steps_per_second': 2.925}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# Load the trained model\n",
    "model = ViTForImageClassification.from_pretrained(base_output_dir)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(base_output_dir)\n",
    "\n",
    "# Define the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")\n",
    "\n",
    "# Now you can run the evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Log and print the evaluation metrics\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)\n",
    "\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
