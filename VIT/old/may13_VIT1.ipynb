{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1500x1500 at 0x21EA5823B90>, 'label': 0, 'filename': 'b0.jpeg'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/may13_VIT1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "\"\"\"\n",
    ".venv/Scripts/activate\n",
    "\n",
    "python -m image_process\n",
    "\"\"\"\n",
    "base_output_dir = f\"models/may13_VIT1\"\n",
    "dataset = load_dataset(\"potato_train/train\")\n",
    "filenames_ds = load_dataset(\"potato_train/train\").cast_column(\"image\", Image(decode=False))\n",
    "\n",
    "filename_col = [x['image']['path'].split('\\\\')[-1] for x in filenames_ds['train']]\n",
    "dataset['train'] = dataset['train'].add_column(\"filename\", filename_col)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "base_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f143290-9281-4f29-92d0-f9013f809519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_convert_rgb\": null,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "# import model\n",
    "model_id = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cfd97ad-58b8-4429-8bac-f0639a92b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2617bbc7-5778-475f-a8fd-3be9fa1d5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomRotation,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    ColorJitter,\n",
    "    RandomAffine\n",
    ")\n",
    "from PIL import Image  # Import PIL for RandomAffine's resample\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])\n",
    "\n",
    "training_transforms = Compose([\n",
    "    Resize(size),\n",
    "    CenterCrop(size),\n",
    "    # RandomRotation((-30, 30)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), interpolation=Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def training_image_preprocess(batch):\n",
    "    batch[\"pixel_values\"] = torch.stack([training_transforms(img) for img in batch[\"image\"]])\n",
    "    return batch\n",
    "\n",
    "def preprocess(batch):\n",
    "    # take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor(\n",
    "        batch['image'],\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4974f0-9cdb-4fa2-90ba-67a69a98b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "dataset_train = train_test_split[\"train\"]\n",
    "dataset_test = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb414c3b-07e5-4478-a882-21dbe846e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " ClassLabel(names=['Bacteria', 'Fungi', 'Healthy', 'Pest', 'Phytopthora', 'Virus'], id=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5514339e-7fcc-4c40-aa3c-8d198423a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training dataset\n",
    "prepared_train = dataset_train.with_transform(training_image_preprocess)\n",
    "# ... and the testing dataset\n",
    "prepared_test = dataset_test.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb978-d838-4ba0-8259-32618f3a1b83",
   "metadata": {},
   "source": [
    "Save images of preprocessed images (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8ef810-fc8e-4ff6-af4d-138072228ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "output_dir = f\"{base_output_dir}/preprocessed_train_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for index, item in enumerate(prepared_train):\n",
    "    if index >= 10:\n",
    "        break\n",
    "    pixel_values = item[\"pixel_values\"]\n",
    "    image = to_pil_image(pixel_values)\n",
    "    label_filename = dataset_train[index][\"filename\"]\n",
    "\n",
    "    name_without_extension, extension = os.path.splitext(label_filename)\n",
    "    filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    image.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0488fd7e-7837-4a6a-bea1-a02a00e3fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{base_output_dir}/preprocessed_test_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for index, item in enumerate(prepared_test):\n",
    "    if index >= 10:\n",
    "        break\n",
    "    pixel_values = item[\"pixel_values\"]\n",
    "    image = to_pil_image(pixel_values)\n",
    "    label_filename = dataset_test[index][\"filename\"]\n",
    "\n",
    "    name_without_extension, extension = os.path.splitext(label_filename)\n",
    "    filename = f\"pp_{name_without_extension}.png\"\n",
    "\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    image.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ab1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=p.label_ids,\n",
    "        )\n",
    "    )\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=p.label_ids, average=\"weighted\"))\n",
    "    return results\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2653db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='models',\n",
    "  per_device_train_batch_size=16,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=8,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=5e-5,\n",
    "  save_total_limit=2,\n",
    "  seed=42,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "labels = dataset_train.features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_id,  # classification head\n",
    "    num_labels=len(labels)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    processing_class=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9b483da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1088' max='1088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1088/1088 30:02, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.713668</td>\n",
       "      <td>0.813653</td>\n",
       "      <td>0.804905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.852399</td>\n",
       "      <td>0.853595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.451610</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.863346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.337100</td>\n",
       "      <td>0.447209</td>\n",
       "      <td>0.845018</td>\n",
       "      <td>0.843741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.395547</td>\n",
       "      <td>0.872694</td>\n",
       "      <td>0.872265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.413418</td>\n",
       "      <td>0.872694</td>\n",
       "      <td>0.872293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.382393</td>\n",
       "      <td>0.883764</td>\n",
       "      <td>0.883073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.372025</td>\n",
       "      <td>0.891144</td>\n",
       "      <td>0.891026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.384194</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>0.877236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.394922</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.884950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          8.0\n",
      "  total_flos               = 1250029893GF\n",
      "  train_loss               =       0.3427\n",
      "  train_runtime            =   0:30:04.25\n",
      "  train_samples_per_second =          9.6\n",
      "  train_steps_per_second   =        0.603\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "# save tokenizer with the model\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "\n",
    "# save the trainer state\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e959365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TahirLidasanJr\\Desktop\\CS_180\\cs180_project_TEMP SANDBOX\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy               =     0.8911\n",
      "  eval_f1                     =      0.891\n",
      "  eval_loss                   =      0.372\n",
      "  eval_model_preparation_time =     0.0009\n",
      "  eval_runtime                = 0:00:38.44\n",
      "  eval_samples_per_second     =     14.096\n",
      "  eval_steps_per_second       =      1.769\n",
      "{'eval_loss': 0.3720252215862274, 'eval_model_preparation_time': 0.0009, 'eval_accuracy': 0.8911439114391144, 'eval_f1': 0.8910257759924213, 'eval_runtime': 38.4497, 'eval_samples_per_second': 14.096, 'eval_steps_per_second': 1.769}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# Load the trained model\n",
    "model = ViTForImageClassification.from_pretrained(base_output_dir)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(base_output_dir)\n",
    "\n",
    "# Define the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=feature_extractor,\n",
    "    eval_dataset=prepared_test,  # Use your evaluation dataset here\n",
    ")\n",
    "\n",
    "# Now you can run the evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Log and print the evaluation metrics\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)\n",
    "\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
